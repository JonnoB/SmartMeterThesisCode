p=1
y =0
(sum(x^p))^(1/p)
x <- c(2, 3, 4)
p=1
y =0
(sum(x^p))^(1/p)
x <- c(2, 3, 4)
p=2
y =0
(sum(x^p))^(1/p)
x <- c(1, 1, 1)
p= c(1,2)
y =0
(sum(x^p))^(1/p)
x <- c(1, 1, 1)
p1= 1
p2 =2
c(  max(x),(sum(x^p2))^(1/p2),  (sum(x^p1))^(1/p1))
x <- c(.1, .1, .1)
p1= 1
p2 =2
c(  max(x),(sum(x^p2))^(1/p2),  (sum(x^p1))^(1/p1))
x <- c(0.9, 0.9, 0.9)
p1= 1
p2 =2
c(  max(x),(sum(x^p2))^(1/p2),  (sum(x^p1))^(1/p1))
x <- c(0.5, 0.5, 0.5)
p1= 1
p2 =2
c(  max(x),(sum(x^p2))^(1/p2),  (sum(x^p1))^(1/p1))
x <- c(0.33, 0.33, 0.33)
p1= 1
p2 =2
c(  max(x),(sum(x^p2))^(1/p2),  (sum(x^p1))^(1/p1))
x <- c(0, 0, 6)
p1= 1
p2 =2
c(  max(x),(sum(x^p2))^(1/p2),  (sum(x^p1))^(1/p1))
x <- 1:8
x2 <- 1 + choose(x,1) + choose(x,2)
x2
sum(x2)
2*prod(x2)
x <- 2:7
x2 <- 1 + choose(x,1) + choose(x,2)
x2
sum(x2)
prod(x2)*2
(6*9+4*5)/10
(6*9+2*5)/8
18*800000
3000000/25
x <- 2:7
x2 <- 1 + choose(x,1) + choose(x,2)
x2
2+sum(x2)
combn(1:4,4)
combn(letters[1:4], 2)
combn(letters[1:4], 3)
combn([1:4], 3)
combn(1:4, 3)
combn(1:4, 4)
combn(1:4, 1)
combn(1:4, 3)
expand.grid(0:1, 0:1, 0:1)
expand.grid(1:4, 1:4, 1:4)
combn(1:4,4)
combn(1:4)
combn(1:4,1)
combn(1:4,2)
combn(1:4,3)
permn(1:4,3)
require(gtools)
permutations(n = 9, r = 3, v = 1:9)
?permutations
permutations(n = 4, r = 4, v = 1:4)
R.Version()
install.packages(c("BH", "car", "caret", "digest", "earth", "ggplot2", "git2r", "htmltools", "knitr", "pbkrtest", "plotrix", "Rcpp", "rJava", "rmarkdown", "rstudioapi", "tidyr"))
install.packages(c("BH", "car", "caret", "digest", "earth", "ggplot2", "git2r", "htmltools", "knitr", "pbkrtest", "plotrix", "Rcpp", "rJava", "rmarkdown", "tidyr"))
1-(1-0.95)^21
1-(1-0.05)^21
log(756)
(0.7-0.45*0.35)/0.65
(0.7-0.45*0.35-0.9*0.15)/0.5
library(dplyr)
data.frame(numbers = 1:20) %>% mutate(log = log(numbers), other = numbers-1)
data.frame(numbers = (1:20)/20) %>% mutate(log = log(numbers), other = numbers-1)
?return
4/30
4/27
3/27
matrix(0, ,6,6)
matrix(0, 6,6)
mat1 <- matrix(0, 6,6)
mat1[-c(5:6), -c(1:2)]
mat1[-c(5:6), -c(1:2)] <- diag(4)
mat1
mat1 <- matrix(0, 6,6)
mat1[-c(5:6), -c(1:2)] <- diag(4)
mat1[1,2] <-1
mat1[5,1] <-1
mat1
mat1 <- matrix(0, 6,6)
mat1[-c(5:6), -c(1:2)] <- diag(4)
mat1[1,2] <-1
mat1[5,6] <-1
mat1
mat1 %*% mat1
mat1 %*% mat1 %*% mat1
mat1 %*% mat1 %*% mat1 %*% mat1
mat2 <- mat1 + t(mat1)
mat2
mat2 %*% mat2 %*% mat1
chord3 <- mat2 %*% mat2 %*% mat1
chord3 <- mat2 %*% mat2 %*% mat2
chord3
four <- chord3 %*% mat2
four
chord3 <- mat2 %*% mat2 #%*% mat2
chord3
chord3 >1
chord
chord3
chord3 >0
chord3 >1
chord3 >0
four
triang <- mat2
mat2
triang[-2, 2]
triang[-2, 2] <-1
triang
triang %*% triang
triang %*% triang %*% triang
triang %*% triang %*% triang %>% triang
triang %*% triang %*% triang %*% triang
mat2 %*% mat2 #%*% mat2
trace(chord3)
?trace
View(triang)
chird3
chord3
mat2 %*% mat2 %*% mat2 %*% mat2 %*% mat2
mat2
mat2 %*% mat2
mat2 %*% mat2 %*% mat2
mat2 %*% mat2 %*% mat2 %*% mat2
mat2 %*% mat2 %*% mat2 %*% mat2 %*% mat2
mat2 %*% mat2 %*% mat2 %*% mat2 %*% mat2 %*% mat2
22/6
22/12
22/11
mat2 %*% mat2
mat2 %*% mat2 %*% mat2
mat2 %*% mat2 %*% mat2 %*% mat2
mat2 %*% mat2
mat2 %*% mat2 %*% mat2 %*% mat2
mat2 %*% mat2 %*% mat2
mat2 %*% mat2
mat2 %*% mat2
mat2 %*% mat2 %*% mat2
mat2 %*% mat2
mat2
mat2 %*% mat2
mat2 %*% mat2 %*% mat2
mat2 %*% mat2 %*% mat2 %*% mat2
mat2 %*% mat2 %*% mat2 %*% mat2 %*% mat2
mh.gamma <- function(n.sims, start, burnin, cand.sd, shape, rate) {
theta.cur <- start
draws <- c()
theta.update <- function(theta.cur, shape, rate) {
theta.can <- rnorm(1, mean = theta.cur, sd = cand.sd)
accept.prob <- dgamma(theta.can, shape = shape, rate = rate)/dgamma(theta.cur,
shape = shape, rate = rate)
if (runif(1) <= accept.prob)
theta.can
else theta.cur
}
for (i in 1:n.sims) {
draws[i] <- theta.cur <- theta.update(theta.cur, shape = shape,
rate = rate)
}
return(draws[(burnin + 1):n.sims])
}
mh.draws <- mh.gamma(10000, start = 1, burnin = 1000, cand.sd = 2, shape = 1.7, rate = 4.4)
mh.draws
0.2/0.25
mat1 <- matrix(0, 6,6)
mat1[-c(5:6), -c(1:2)] <- diag(4)
mat1[1,2] <-1
mat1[5,6] <-1
mat1
mat1^2
mat2 %*% mat2 %*% mat2 %*% mat2
mat2 <- mat1 + diag(6)
mat2
mat2 %*% mat2 %*% mat2 %*% mat2 %*% mat2
mat1^2mat1
mat1
mat2 <- mat1 + t(mat1) #diag(6)
chord3 <- mat2 %*% mat2 %*% mat2 %*% mat2 %*% mat2
chord3 <- mat2 %*% mat2 %*% mat2 %*% mat2
chord3 <- chord3 +chord3 %*% mat2
chord3
mat2 <- mat1
chord3 <- mat2 %*% mat2 %*% mat2 %*% mat2
chord3 <- chord3 +chord3 %*% mat2
chord3
bigcor <- function(x, nblocks = 10, verbose = TRUE, par = TRUE, ...)
{
library(ff, quietly = TRUE)
NCOL <- ncol(x)
#sets saving options to the current directory for the temp file this prevents
#errors when using server based systems such as AWS that have write restrictions
## preallocate square matrix of dimension
## ncol(x) in 'ff' single format
corMAT <- ff(vmode = "single", dim = c(NCOL, NCOL))
print("Converted to ff matrix")
remainder <- nblocks-(NCOL %%nblocks)
groupsize <-(NCOL +remainder)/nblocks
## split column numbers into 'nblocks' groups
SPLIT <- split(1:NCOL, rep(1:nblocks,each=groupsize)[1:NCOL])
## create all unique combinations of blocks
COMBS <- expand.grid(1:length(SPLIT), 1:length(SPLIT))
COMBS <- t(apply(COMBS, 1, sort))
COMBS <- unique(COMBS)
print("unique combinations identified")
## iterate through each block combination, calculate correlation matrix
## between blocks and store them in the preallocated matrix on both
## symmetric sides of the diagonal
if(par){
mclapply(1:nrow(COMBS), function(i){
COMB <- COMBS[i, ]
G1 <- SPLIT[[COMB[1]]]
G2 <- SPLIT[[COMB[2]]]
if (verbose) cat("Block", COMB[1], "with Block", COMB[2], "\n")
flush.console()
COR <- cor(x[, G1], x[, G2], ...)
corMAT[G1, G2] <- COR
corMAT[G2, G1] <- t(COR)
COR <- NULL
},
mc.cores = detectCores())
} else {
for (i in 1:nrow(COMBS)) {
COMB <- COMBS[i, ]
G1 <- SPLIT[[COMB[1]]]
G2 <- SPLIT[[COMB[2]]]
if (verbose) cat("Block", COMB[1], "with Block", COMB[2], "\n")
flush.console()
COR <- cor(x[, G1], x[, G2], ...)
corMAT[G1, G2] <- COR
corMAT[G2, G1] <- t(COR)
COR <- NULL
}
}
gc()
return(corMAT)
}
#taken from http://www.r-bloggers.com/bigcor-large-correlation-matrices-in-r/
#It's questionable weather there is a point to parallelising this,
#as if you have enough RAM you don't need bigcor
bigcor <- function(x, nblocks = 10, verbose = TRUE, par = TRUE, ...)
{
library(ff, quietly = TRUE)
NCOL <- ncol(x)
#sets saving options to the current directory for the temp file this prevents
#errors when using server based systems such as AWS that have write restrictions
## preallocate square matrix of dimension
## ncol(x) in 'ff' single format
corMAT <- ff(vmode = "single", dim = c(NCOL, NCOL))
print("Converted to ff matrix")
remainder <- nblocks-(NCOL %%nblocks)
groupsize <-(NCOL +remainder)/nblocks
## split column numbers into 'nblocks' groups
SPLIT <- split(1:NCOL, rep(1:nblocks,each=groupsize)[1:NCOL])
## create all unique combinations of blocks
COMBS <- expand.grid(1:length(SPLIT), 1:length(SPLIT))
COMBS <- t(apply(COMBS, 1, sort))
COMBS <- unique(COMBS)
print("unique combinations identified")
## iterate through each block combination, calculate correlation matrix
## between blocks and store them in the preallocated matrix on both
## symmetric sides of the diagonal
if(par){
mclapply(1:nrow(COMBS), function(i){
COMB <- COMBS[i, ]
G1 <- SPLIT[[COMB[1]]]
G2 <- SPLIT[[COMB[2]]]
if (verbose) cat("Block", COMB[1], "with Block", COMB[2], "\n")
flush.console()
COR <- cor(x[, G1], x[, G2], ...)
corMAT[G1, G2] <- COR
corMAT[G2, G1] <- t(COR)
COR <- NULL
},
mc.cores = detectCores())
} else {
for (i in 1:nrow(COMBS)) {
COMB <- COMBS[i, ]
G1 <- SPLIT[[COMB[1]]]
G2 <- SPLIT[[COMB[2]]]
if (verbose) cat("Block", COMB[1], "with Block", COMB[2], "\n")
flush.console()
COR <- cor(x[, G1], x[, G2], ...)
corMAT[G1, G2] <- COR
corMAT[G2, G1] <- t(COR)
COR <- NULL
}
}
gc()
return(corMAT)
}
#taken from http://www.r-bloggers.com/bigcor-large-correlation-matrices-in-r/
#It's questionable weather there is a point to parallelising this,
#as if you have enough RAM you don't need bigcor
bigcor <- function(x, nblocks = 10, verbose = TRUE, par = FALSE, ...)
{
library(ff, quietly = TRUE)
NCOL <- ncol(x)
#sets saving options to the current directory for the temp file this prevents
#errors when using server based systems such as AWS that have write restrictions
## preallocate square matrix of dimension
## ncol(x) in 'ff' single format
corMAT <- ff(vmode = "single", dim = c(NCOL, NCOL))
print("Converted to ff matrix")
remainder <- nblocks-(NCOL %%nblocks)
groupsize <-(NCOL +remainder)/nblocks
## split column numbers into 'nblocks' groups
SPLIT <- split(1:NCOL, rep(1:nblocks,each=groupsize)[1:NCOL])
## create all unique combinations of blocks
COMBS <- expand.grid(1:length(SPLIT), 1:length(SPLIT))
COMBS <- t(apply(COMBS, 1, sort))
COMBS <- unique(COMBS)
print("unique combinations identified")
## iterate through each block combination, calculate correlation matrix
## between blocks and store them in the preallocated matrix on both
## symmetric sides of the diagonal
if(par){
mclapply(1:nrow(COMBS), function(i){
COMB <- COMBS[i, ]
G1 <- SPLIT[[COMB[1]]]
G2 <- SPLIT[[COMB[2]]]
if (verbose) cat("Block", COMB[1], "with Block", COMB[2], "\n")
flush.console()
COR <- cor(x[, G1], x[, G2], ...)
corMAT[G1, G2] <- COR
corMAT[G2, G1] <- t(COR)
COR <- NULL
},
mc.cores = detectCores())
} else {
for (i in 1:nrow(COMBS)) {
COMB <- COMBS[i, ]
G1 <- SPLIT[[COMB[1]]]
G2 <- SPLIT[[COMB[2]]]
if (verbose) cat("Block", COMB[1], "with Block", COMB[2], "\n")
flush.console()
COR <- cor(x[, G1], x[, G2], ...)
corMAT[G1, G2] <- COR
corMAT[G2, G1] <- t(COR)
COR <- NULL
}
}
gc()
return(corMAT)
}
#taken from http://www.r-bloggers.com/bigcor-large-correlation-matrices-in-r/
setwd(daytimeseries)
list.vertex.attributes(graph3)
?hms
library(lubridate)
?hms
packages <-c("stringr", "lubridate", "data.table","caret", "xgboost","e1071", "R.utils", "corrplot", "Hmisc", "Amelia", "Matrix", "ff", "ggdendro", "zoo", "networkD3", "igraph","parallel", "magrittr", "ggplot2", "tidyr", "xtable","dplyr")
new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
sapply(packages, library, character.only = TRUE)
rm(list=c("packages",  "new.packages"))
isAWS <-(Sys.info()[1]=="Linux")
# getwd()
# setwd("/home/rstudio")
# dir.create("Cormats")
# dir.create("graphs")
file.path("/home/rstudio", "Cormats")
file.path("/home/rstudio", "graphs")
```
Project Folders
```{r Paths}
#basewd needs to be changed
if(isAWS){
basewd<- "/home/rstudio/Dropbox/Thesis-Data"
Figures <- "/home/rstudio/Dropbox/Apps/ShareLaTeX/University College London thesis/Figures"
Figures <- file.path("/home/rstudio/Dropbox/Apps/ShareLaTeX/University-College-London-thesis/Figures")
TexTables <- file.path("/home/rstudio/Dropbox/Apps/ShareLaTeX/University-College-London-thesis/Tables")
functioncode <- file.path(basewd, "SmartMeterThesisCode","Functions")
Cormats <- "/home/rstudio/Cormats"
GraphPath<-file.path("/home/rstudio", "graphs")
options(fftempdir = "/home/rstudio")
} else {
basewd <- "C:/Users/pc1/Dropbox/Thesis-Data"
Figures <- file.path("C:/Users/pc1/Dropbox/Apps/ShareLaTeX/University-College-London-thesis/Figures")
TexTables <- file.path("C:/Users/pc1/Dropbox/Apps/ShareLaTeX/University-College-London-thesis/Tables")
functioncode <- "C:/Users/pc1/Dropbox/Thesis-Data/SmartMeterThesisCode/Functions"
Cormats <- "C:/Users/pc1/Dropbox/Thesis-Data/Cormats"
}
SubDataSets <- file.path(basewd, "SubDataSets")
datafile <- file.path(basewd, "TCa1")
daytimeseries <-file.path(basewd,"Cleandata")
#file.path(basewd,"Cormats")
```
Source functions
```{r Functions}
setwd(functioncode)
sapply(list.files(), source)
```
setwd(SubDataSets)
SankeyFrame<-readRDS("SankeyFrame.rds") %>% as.data.frame
Clustconversion <- readRDS("Clustconversion.rds")
View(Clustconversion)
SankeyData(95:101,SankeyFrame, Clustconversion, SOUP = TRUE)
View(Clustconversion)
Clustconversion$day <=3
test <-SankeyFrame[Clustconversion$day <=3,Clustconversion$day <=3]
View(test)
test <-matrix(1, ncol(SankeyFrame), ncol(SankeyFrame))
Clustconversion$day[Clustconversion$day <=3]
test <-matrix(0, length(IDvect), length(IDvect))
IDvect <- Clustconversion$day[Clustconversion$day <=3]
test <-matrix(0, length(IDvect), length(IDvect))
test[IDvect==1,IDvect==2]<-1
test[IDvect==2,IDvect==3]<-1
View(test)
IDvect <- Clustconversion$day[Clustconversion$day <=3]
test <-matrix(0, length(IDvect), length(IDvect))
for( i in 2:3){
test[IDvect==i+1,IDvect==i]<-1
}
View(test)
test <-matrix(0, length(IDvect), length(IDvect))
sum(test)
for( i in 2:3){
test[IDvect==i+1,IDvect==i]<-1
}
sum(test)
test <-matrix(0, length(IDvect), length(IDvect))
for( i in 2:3){
test[IDvect==i-1,IDvect==i]<-1
}
View(test)
blocks <- 6
IDvect <- Clustconversion$day[Clustconversion$day <=blocks]
test <-matrix(0, length(IDvect), length(IDvect))
for( i in 2:blocks){
test[IDvect==i-1,IDvect==i]<-1
}
orderedheat(test, order = "none")
orderedheat(t(test), order = "none")
orderedheat(t(test), order = "none", merge = 1)
maskex <- orderedheat(t(test), order = "none", merge = 1)
maskex+
ggtitle("Example masking matrix") +
theme(legend.position="none")
View(bigheat)
View(bigheat)
maskex+
ggtitle("Example masking matrix") +
theme(legend.position="none", axis.text.x  = element_blank(), axis.text.y = element_blank())
maskex+
ggtitle("Example masking matrix") +
theme(legend.position="none",
axis.text.x  = element_blank(),
axis.text.y = element_blank(),
axis.title.y="Source Clusters",
axis.title.y="Target Clusters")
maskex+
ggtitle("Example masking matrix") +
theme(legend.position="none",
axis.text.x  = element_blank(),
axis.text.y = element_blank(),
axis.title.y=element_text("Source Clusters"),
axis.title.y=element_text("Target Clusters"))
maskex+
ggtitle("Example masking matrix") +
theme(legend.position="none",
axis.text.x  = element_blank(),
axis.text.y = element_blank())+
xlabs("Target Clusters") +
ylabs("Source Clusters")
maskex+
ggtitle("Example masking matrix") +
theme(legend.position="none",
axis.text.x  = element_blank(),
axis.text.y = element_blank())
maskex+
ggtitle("Example masking matrix") +
theme(legend.position="none",
axis.text.x  = element_blank(),
axis.text.y = element_blank())
+   xlab("Target Clusters") +
ylab("Source Clusters")
maskex+
ggtitle("Example masking matrix") +
theme(legend.position="none",
axis.text.x  = element_blank(),
axis.text.y = element_blank())+
xlab("Target Clusters") +
ylab("Source Clusters")
setwd(file.path(Figures, "Method"))
ggsave("ClusterMask.pdf")
setwd(functioncode)
list.files()
