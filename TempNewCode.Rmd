---
title: "Untitled"
author: "Jonathan Bourne"
date: "24 August 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

#new correlation package useful for this work
https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr

#setup block

```{r}
#This is blocked out I will add additional packages as necessary
#packages <-c("stringr", "lubridate", "data.table","caret", "xgboost","e1071", "R.utils", "corrplot", "Hmisc", "Amelia", "Matrix", "ff", "ggdendro", "zoo", "networkD3", "igraph","parallel", "magrittr", "ggplot2", "tidyr", "xtable","entropy","dplyr", "microbenchmark", "FeatureHashing")


packages <- c("tidyverse", "stringr", "readr", "zoo", "lubridate", "data.table", "ff", "igraph", "microbenchmark")
#file.path("/media/jonno/Seagate Expansion Drive/Msc-Thesis-Data/Cormats")

new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

sapply(packages, library, character.only = TRUE)
rm(list=c("packages",  "new.packages"))

select <- dplyr::select


rootwd <-getwd()
DropboxPath <- file.path(rootwd, "Dropbox")
MScThesis <-file.path(DropboxPath, "MScThesis")
figurespath <- file.path(MsCThesis, "Figures")
ZipData <- file.path(rootwd, "Dropbox", "MScThesis")
daytimeseries <- file.path(MScThesis, "DayTimeSeries")
SavedData <- file.path("/media/jonno/Seagate Expansion Drive/Msc-Thesis-Data")
ModelBlock <- file.path(file.path(SavedData, "ModelBlock"))

#GraphPath <- file.path("/media/jonno/Seagate Expansion Drive/Msc-Thesis-Data/Graphs/WholeDay")


#library("RStudioAMI")
#includeSyncDropbox("BigHeat")
#excludeSyncDropbox("BigHeat")

list.files(file.path(DropboxPath,"SmartMeterThesisCode/Functions"), full.names = T) %>% map(~{source(.x)})
list.files(file.path(DropboxPath, "SmartRtimes/Functions"), full.names = T) %>% map(~{source(.x)})
list.files(file.path(DropboxPath, "BigHeat/Functions"), full.names = T) %>% map(~{source(.x)})

mape <- function(yhat, y){
mean(abs((y - yhat)/y), na.rm = TRUE)}

```


#Load the data

It takes a long time to load and a long time to convert to time. 10.1 gb at max about 2.7 after spreading
```{r}
if(!file.exists("smartmeterdata.rds")){
#Decompress is necessary
if(!file.exists("TC1a")){
  print("Decompressing file")
  decompress_file(ZipData, "TC1a.zip",rootwd)
}
smartdata <- LoadAllSmartMeterData(SmartMeterPath = file.path("TC1a", "TrialMonitoringDataHH.csv"), 
                                   TimeZonePath= file.path("TC1a","HalfHourlyDataSource.csv"))

smartdata <- dcast(smartdata, Date.Time ~`Location ID`,
                     value.var = "Parameter",
                     drop=FALSE)

saveRDS(smartdata, "smartmeterdata.rds")
}else{
  print("Loading smartmeter data")
  smartdata<- readRDS("smartmeterdata.rds")
}



```

##Visualise the NAs
```{r}
isnaframe <- 1-is.na(smartdata[,-1])*1

#How much data is missing as a percentage of total
1-sum(isnaframe)/(ncol(isnaframe)*nrow(isnaframe))

if(file.exists("isnaordering.rds")){
ordering <- createorder(isnaframe, order="column", simMat= FALSE,xblocks=15, yblocks=15, par=FALSE)
saveRDS(ordering,"isnaordering.rds")
}else{
  ordering <- readRDS("isnaordering.rds")
}

IsNaordered <- bigheat(t(isnaframe[,ordering$Colorder]), #transposed so that time is on the x-axis
                mergex = 25, mergey = 5, mid=0.5, legend="Percent Valid")

IsNaordered+
    labs(x = "Date time",
         y = "Smartmeter ID") + ggtitle("Missing data pre-cleaning")
ggsave(file.path(figurespath, "Precleaningmissing.pdf"))

```



#Choose target timeperiod

the days that will be included in the analysis

```{r}
findbreak<- t(isnaframe[, ordering$Colorder]) %>% as.data.frame %>%
  mutate(rowsum = rowSums(.)/nrow(isnaframe), 
         diff= lag(rowsum,1), 
         rowID= 1:nrow(.), 
         rM=(rowsum + lag(rowsum)+lead(rowsum))/3) %>% 
  select(rowsum, diff, rowID, rM)

ggplot(findbreak, aes(x = rowID, y = rowsum)) + geom_line() +
  ggtitle("Identifying break points in the smartmeter clusters") +
  labs(x="Cluster ordered meter IDs", y = "Percent of valid data points")+ 
  scale_y_continuous(labels = scales::percent)
ggsave(file.path(figurespath, "breakpoints.pdf"))

findbreak %>%
  filter(rowID>1650, rowID<6100, rowsum>0.9) %>%
ggplot(., aes(x = rowID, y = rowsum)) + geom_line() 

#Only take data from the big central block of high quality smartmeters that have at least 90\% valid data.
HighQualitySmartMeters <- findbreak %>%
  filter(rowID>1650, rowID<6100, rowsum>0.9)

```


##Fill missing

```{r}
cleandata<- smartdata[,ordering$Colorder[HighQualitySmartMeters$rowID]] %>%
  FillMissingData() %>%
  select(Date.Time, everything())


#there are few coloumns that still have na's
table(colSums(is.na(cleandata)))
#we remove them here
cleandata <- cleandata[,colSums(is.na(cleandata))==0] 
sum(is.na(cleandata))

#One column has gone bonkers I will remove it for simplicity
test <- cleandata[,2] %>% as.numeric()
cleandata[,2][which(is.na(test))]


cleandata <- cleandata[,-2] %>%
  filter(date(Date.Time) != ymd("2011-04-30")) #this date only has two time periods and so needs to be removed

saveRDS(cleandata, file ="cleandatafilled.rds")

SaveAsDay(cleandata, daytimeseries)

```


##Calculate internal corellation
```{r}
set.seed(1238)
NodeIDs <- sample(1:(ncol(cleandata)-1), 50)+1
IntCor <- cleandata[,c(1,NodeIDs)] %>%
  mutate(Time = paste(hour(cleandata$Date.Time),
                    minute(cleandata$Date.Time),
                    sep=":"),
         Date = as.Date(Date.Time)
)%>%  gather(key = NodeID, value = kwh, 
             -Date.Time, 
             -Time,
             -Date) %>% select(-Date.Time) %>%
  spread(key= Time, value=kwh)

IntCorList <- map(unique(IntCor$NodeID), ~{
  
  IntCor %>% filter(NodeID ==.x) %>% select(-NodeID, -Date) %>%
    t %>%
    cor
  })


```

#Choose edge corellation
Chooses the edge corellation for each of the model versions

```{r}

TimePeriods <- data_frame(Start = c("00:00:00","00:00:00","12:00:00", "00:00:00", "06:00:00", "12:00:00", "18:00:00" ),
           End= c("24:00:00", "12:00:00","24:00:00","06:00:00", "12:00:00", "18:00:00", "24:00:00" ),
           daypart = c("WholeDay", "Morning12", "Afternoon12", "EarlyMorning6", "Morning6", "Afternoon6", "Evening6"))

if(file.exists(file.path(SavedData, "TimeAnalysis.rds"))){
  TimeAnalysis <- readRDS(file.path(MScThesis, "TimeAnalysis.rds"))
} else{


TimeAnalysis <- 1:nrow(TimePeriods) %>%
  map_df(~{
    
    periodchar <-paste(TimePeriods$Start[.x], TimePeriods$End[.x], sep = "-")
    print(periodchar)
    GetCutoffReduction(StartTime = TimePeriods$Start[.x], EndTIme = TimePeriods$End[.x], samples = 30) %>%
      mutate(period = periodchar,
             daypart = TimePeriods$daypart[.x])
  })
  saveRDS(TimeAnalysis, file.path(SavedData, "TimeAnalysis.rds"))

}

CutoffDiff  <- TimeAnalysis %>%
  select(type, period, cutoff, value) %>%
  spread(key = type, value) %>%
  mutate(diff = nodes-edges) 

#maximum difference by period
CutoffDiff %>%
  group_by(period) %>%
  filter(diff == max(diff))


CutoffDiff %>%
  ggplot(aes(x = cutoff, y = diff, colour = period)) + geom_line() +   
  scale_y_continuous(labels = scales::percent)

```

#Create all graphs using the cutoffs previously selected

#Create graphs and cluster
```{r}
TimePeriods <- TimePeriods %>% arrange(paste(Start, End, sep = "-")) %>%
  mutate(cutoff = CutoffDiff %>%
  group_by(period) %>%
  filter(diff == max(diff)) %>% pull(cutoff),
         cutoff2 = cutoff - 0.05)

1:nrow(TimePeriods) %>% walk(~{
  
CreateAllGraphs2(SourceFolder = daytimeseries, 
                 TargetFolder = file.path(ModelBlock, TimePeriods2$daypart[.x], "Graphs"),
                 cutoff = TimePeriods2$cutoff2[.x],
                 StartTime = TimePeriods2$Start[.x],
                 EndTIme = TimePeriods2$End[.x])

})

rm(TimePeriods2)

```


#Split point

Everything above this point is common data processing. Below this point the processing is dependent on the time of day to be analyzed


##Choose clustering algorithm
In the code thiss done after the actual graphs are created but just demonstrates why I chose the algo I did

The results of the chunk show that all the methods have a similar modularity, Louvain is slightly higher, but that the Louvain method is considerably faster

```{r}
setwd(Cormats)

setwd(GraphPath)
graphvect <- sample(list.files(),30)

CommTimes<-lapply(1:30, function(n){
    graph <- read.graph(graphvect[n], format = "graphml")  
    print(n)
    fc <-fastgreedy.community(graph)
    wc <-walktrap.community(graph)
    ic <- infomap.community(graph)
    lc <- cluster_louvain(graph)
    mod<- data.frame( 
      Fastgreedy= modularity(fc),
      Walktrap= modularity(wc),
      Infomap = modularity(ic),
      Louvain =modularity(lc))
    }

)%>% bind_rows()  
  
CommTimes <- CommTimes %>% gather(., key= Method, value= Modularity) %>% 
  mutate(Method= factor(Method, 
                          labels = c("Fastgreedy", 
                                   "Infomap",
                                   "Louvain",
                                     "Walktrap" 
                                   ))
  )

CommTimes %>% ggplot(., aes(x=Method, y=Modularity, fill= Method)) + geom_boxplot() +theme(legend.position="none") +
  ggtitle("Modularity results for different\n Community detection Algorithms")+
   scale_color_manual(values= ggplotColours(4), guide=FALSE)

setwd(GraphPath)
graph <- read.graph(graphvect[1], format = "graphml")  
bench <- microbenchmark(    fc <-fastgreedy.community(graph),
            wc <-walktrap.community(graph),
            ic <- infomap.community(graph),
            lc <- cluster_louvain(graph),
             times = 30)

bench <-bench %>% rename(Method=expr) %>% 
  mutate(time=(time)/1*10^-9, Method= factor(Method, 
                          levels= levels(Method)[order(levels(Method))],
                          labels = c("Fastgreedy", 
                                   "Infomap",
                                   "Louvain",
                                     "Walktrap" 
                                   )
                          )
         ) 


bench %>% ggplot(., aes(x=Method, y=time, fill= Method)) + geom_boxplot() +theme(legend.position="none") +
  labs(title= "Benchmarking Algorithm run time", y="Run time in seconds") +
  scale_color_manual(values= ggplotColours(4), guide=FALSE)


rm(list = "CommTimes", "graph", "bench")
```

#Assign Cluster Family ID

```{r}

TimePeriods$daypart %>% walk(~{
  print(.x)
  daypart <- .x
  
  print(daypart)
  source(file = file.path(DropboxPath,"SmartMeterThesisCode/SubCode", "PredictionProcess.R"))

  }
)

for(i in 1:nrow(TimePeriods)){
  
  daypart <- TimePeriods$daypart[i]
  StartTimei <- TimePeriods$Start[i]
  EndTimei <- TimePeriods$End[i]
  cutoffi <- TimePeriods$cutoff2[i]
  
  print(TimePeriods[i,])

  
  source(file = file.path(DropboxPath,"SmartMeterThesisCode/SubCode", "PredictionProcess.R"))

  }


#The plots show that the 24 hour clusters are broad time of day clusters
test <- melt(clusterprofile, id ="time", variable.name = "UniqueID", value.name = "kwh" ) %>%
  left_join(., select(Clustconversion, UniqueID, ClustID), 
            by = "UniqueID") 
test %>% 
  group_by(ClustID, time) %>% 
  summarise(mean= mean(kwh), median=median(kwh), sd=sd(kwh), sum=sum(kwh)) %>% 
  ungroup %>%
  ggplot( aes(x=time, y=mean, group= ClustID, colour= ClustID)) + geom_line() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

test %>% 
  filter(ClustID !="soup") %>%
     ggplot(., aes(x=time,y=kwh, group=UniqueID,colour=ClustID)) +
     geom_line(alpha=0.1)+
     facet_grid(ClustID~.) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

```

```{r}


Pred2 <- PredictedProfiles %>%
  select(-ClustID) %>%
  group_by(time, date) %>%
  summarise_all(sum, na.rm = T)

PredictedProfiles %>%
  ggplot(aes(x = time, y = kwh, group = paste(ClustID, date),  colour = ClustID))+ geom_line() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

PredictedProfiles %>%
  ggplot(aes(x = time, y = TrueValues, group = paste(ClustID, date),  colour = ClustID))+ geom_line() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

metrics(data = Pred2, truth = TrueValues, estimate = kwh) %>% 
  mutate(mape =  mape(Pred2$kwh, Pred2$TrueValues))

```

