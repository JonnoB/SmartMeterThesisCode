---
title: "Untitled"
author: "Jonathan Bourne"
date: "24 August 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

#new correlation package useful for this work
https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr


The Create CLuster graph function is not working properly the graphs are not getting clusters I don't know why

#setup block

```{r}
#This is blocked out I will add additional packages as necessary
#packages <-c("stringr", "lubridate", "data.table","caret", "xgboost","e1071", "R.utils", "corrplot", "Hmisc", "Amelia", "Matrix", "ff", "ggdendro", "zoo", "networkD3", "igraph","parallel", "magrittr", "ggplot2", "tidyr", "xtable","entropy","dplyr", "microbenchmark", "FeatureHashing")


packages <- c("tidyverse", "stringr", "readr", "zoo", "lubridate", "data.table", "ff", "igraph", "microbenchmark", "yardstick")
#file.path("/media/jonno/Seagate Expansion Drive/Msc-Thesis-Data/Cormats")

new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

sapply(packages, library, character.only = TRUE)
rm(list=c("packages",  "new.packages"))

select <- dplyr::select


rootwd <-getwd()
DropboxPath <- file.path(rootwd, "Dropbox")
MScThesis <-file.path(DropboxPath, "MScThesis")
figurespath <- file.path(MsCThesis, "Figures")
ZipData <- file.path(rootwd, "Dropbox", "MScThesis")
daytimeseries <- file.path(MScThesis, "DayTimeSeries")
SavedData <- file.path("/media/jonno/Seagate Expansion Drive/Msc-Thesis-Data")
ModelBlock <- file.path(file.path(SavedData, "ModelBlock"))

#GraphPath <- file.path("/media/jonno/Seagate Expansion Drive/Msc-Thesis-Data/Graphs/WholeDay")


#library("RStudioAMI")
#includeSyncDropbox("BigHeat")
#excludeSyncDropbox("BigHeat")

list.files(file.path(DropboxPath,"SmartMeterThesisCode/Functions"), full.names = T) %>% map(~{source(.x)})
list.files(file.path(DropboxPath, "SmartRtimes/Functions"), full.names = T) %>% map(~{source(.x)})
list.files(file.path(DropboxPath, "BigHeat/Functions"), full.names = T) %>% map(~{source(.x)})

mape <- function(yhat, y){
mean(abs((y - yhat)/y), na.rm = TRUE)}

```


#Load the data

It takes a long time to load and a long time to convert to time. 10.1 gb at max about 2.7 after spreading
```{r}
if(!file.exists("smartmeterdata.rds")){
#Decompress is necessary
if(!file.exists("TC1a")){
  print("Decompressing file")
  decompress_file(ZipData, "TC1a.zip",rootwd)
}
smartdata <- LoadAllSmartMeterData(SmartMeterPath = file.path("TC1a", "TrialMonitoringDataHH.csv"), 
                                   TimeZonePath= file.path("TC1a","HalfHourlyDataSource.csv"))

smartdata <- dcast(smartdata, Date.Time ~`Location ID`,
                     value.var = "Parameter",
                     drop=FALSE)

saveRDS(smartdata, "smartmeterdata.rds")
}else{
  print("Loading smartmeter data")
  smartdata<- readRDS("smartmeterdata.rds")
}



```

##Visualise the NAs
```{r}
isnaframe <- 1-is.na(smartdata[,-1])*1

#How much data is missing as a percentage of total
1-sum(isnaframe)/(ncol(isnaframe)*nrow(isnaframe))

if(file.exists("isnaordering.rds")){
ordering <- createorder(isnaframe, order="column", simMat= FALSE,xblocks=15, yblocks=15, par=FALSE)
saveRDS(ordering,"isnaordering.rds")
}else{
  ordering <- readRDS("isnaordering.rds")
}

IsNaordered <- bigheat(t(isnaframe[,ordering$Colorder]), #transposed so that time is on the x-axis
                mergex = 25, mergey = 5, mid=0.5, legend="Percent Valid")

IsNaordered+
    labs(x = "Date time",
         y = "Smartmeter ID") + ggtitle("Missing data pre-cleaning")
ggsave(file.path(figurespath, "Precleaningmissing.pdf"))

```



#Choose target timeperiod

the days that will be included in the analysis

```{r}
findbreak<- t(isnaframe[, ordering$Colorder]) %>% as.data.frame %>%
  mutate(rowsum = rowSums(.)/nrow(isnaframe), 
         diff= lag(rowsum,1), 
         rowID= 1:nrow(.), 
         rM=(rowsum + lag(rowsum)+lead(rowsum))/3) %>% 
  select(rowsum, diff, rowID, rM)

ggplot(findbreak, aes(x = rowID, y = rowsum)) + geom_line() +
  ggtitle("Identifying break points in the smartmeter clusters") +
  labs(x="Cluster ordered meter IDs", y = "Percent of valid data points")+ 
  scale_y_continuous(labels = scales::percent)
ggsave(file.path(figurespath, "breakpoints.pdf"))

findbreak %>%
  filter(rowID>1650, rowID<6100, rowsum>0.9) %>%
ggplot(., aes(x = rowID, y = rowsum)) + geom_line() 

#Only take data from the big central block of high quality smartmeters that have at least 90\% valid data.
HighQualitySmartMeters <- findbreak %>%
  filter(rowID>1650, rowID<6100, rowsum>0.9)

```


##Fill missing

```{r}
cleandata<- smartdata[,ordering$Colorder[HighQualitySmartMeters$rowID]] %>%
  FillMissingData() %>%
  select(Date.Time, everything())


#there are few coloumns that still have na's
table(colSums(is.na(cleandata)))
#we remove them here
cleandata <- cleandata[,colSums(is.na(cleandata))==0] 
sum(is.na(cleandata))

#One column has gone bonkers I will remove it for simplicity
test <- cleandata[,2] %>% as.numeric()
cleandata[,2][which(is.na(test))]


cleandata <- cleandata[,-2] %>%
  filter(date(Date.Time) != ymd("2011-04-30")) #this date only has two time periods and so needs to be removed

saveRDS(cleandata, file ="cleandatafilled.rds")

SaveAsDay(cleandata, daytimeseries)

```


##Calculate internal corellation
```{r}
set.seed(1238)
NodeIDs <- sample(1:(ncol(cleandata)-1), 50)+1
IntCor <- cleandata[,c(1,NodeIDs)] %>%
  mutate(Time = paste(hour(cleandata$Date.Time),
                    minute(cleandata$Date.Time),
                    sep=":"),
         Date = as.Date(Date.Time)
)%>%  gather(key = NodeID, value = kwh, 
             -Date.Time, 
             -Time,
             -Date) %>% select(-Date.Time) %>%
  spread(key= Time, value=kwh)

IntCorList <- map(unique(IntCor$NodeID), ~{
  
  IntCor %>% filter(NodeID ==.x) %>% select(-NodeID, -Date) %>%
    t %>%
    cor
  })


```

#Choose edge corellation
Chooses the edge corellation for each of the model versions

```{r}

TimePeriods <- data_frame(Start = c("00:00:00","00:00:00","12:00:00", "00:00:00", "06:00:00", "12:00:00", "18:00:00" ),
           End= c("24:00:00", "12:00:00","24:00:00","06:00:00", "12:00:00", "18:00:00", "24:00:00" ),
           daypart = c("WholeDay", "Morning12", "Afternoon12", "EarlyMorning6", "Morning6", "Afternoon6", "Evening6"))

if(file.exists(file.path(SavedData, "TimeAnalysis.rds"))){
  TimeAnalysis <- readRDS(file.path(SavedData, "TimeAnalysis.rds"))
} else{


TimeAnalysis <- 1:nrow(TimePeriods) %>%
  map_df(~{
    
    periodchar <-paste(TimePeriods$Start[.x], TimePeriods$End[.x], sep = "-")
    print(periodchar)
    GetCutoffReduction(StartTime = TimePeriods$Start[.x], EndTIme = TimePeriods$End[.x], samples = 30) %>%
      mutate(period = periodchar,
             daypart = TimePeriods$daypart[.x])
  })
  saveRDS(TimeAnalysis, file.path(SavedData, "TimeAnalysis.rds"))

}

CutoffDiff  <- TimeAnalysis %>%
  select(type, period, cutoff, value) %>%
  spread(key = type, value) %>%
  mutate(diff = nodes-edges) 

#maximum difference by period
CutoffDiff %>%
  group_by(period) %>%
  filter(diff == max(diff))


CutoffDiff %>%
  ggplot(aes(x = cutoff, y = diff, colour = period)) + geom_line() +   
  scale_y_continuous(labels = scales::percent)

```

#Create all graphs using the cutoffs previously selected

#Create graphs and cluster
```{r}
TimePeriods <- TimePeriods %>% arrange(paste(Start, End, sep = "-")) %>%
  mutate(cutoff = CutoffDiff %>%
  group_by(period) %>%
  filter(diff == max(diff)) %>% pull(cutoff),
         cutoff2 = cutoff - 0.05)

1:nrow(TimePeriods) %>% walk(~{
  
CreateAllGraphs2(SourceFolder = daytimeseries, 
                 TargetFolder = file.path(ModelBlock, TimePeriods2$daypart[.x], "Graphs"),
                 cutoff = TimePeriods2$cutoff2[.x],
                 StartTime = TimePeriods2$Start[.x],
                 EndTIme = TimePeriods2$End[.x])

})

rm(TimePeriods2)

```


#Split point

Everything above this point is common data processing. Below this point the processing is dependent on the time of day to be analyzed


##Choose clustering algorithm
In the code thiss done after the actual graphs are created but just demonstrates why I chose the algo I did

The results of the chunk show that all the methods have a similar modularity, Louvain is slightly higher, but that the Louvain method is considerably faster

```{r}
setwd(Cormats)

setwd(GraphPath)
graphvect <- sample(list.files(),30)

CommTimes<-lapply(1:30, function(n){
    graph <- read.graph(graphvect[n], format = "graphml")  
    print(n)
    fc <-fastgreedy.community(graph)
    wc <-walktrap.community(graph)
    ic <- infomap.community(graph)
    lc <- cluster_louvain(graph)
    mod<- data.frame( 
      Fastgreedy= modularity(fc),
      Walktrap= modularity(wc),
      Infomap = modularity(ic),
      Louvain =modularity(lc))
    }

)%>% bind_rows()  
  
CommTimes <- CommTimes %>% gather(., key= Method, value= Modularity) %>% 
  mutate(Method= factor(Method, 
                          labels = c("Fastgreedy", 
                                   "Infomap",
                                   "Louvain",
                                     "Walktrap" 
                                   ))
  )

CommTimes %>% ggplot(., aes(x=Method, y=Modularity, fill= Method)) + geom_boxplot() +theme(legend.position="none") +
  ggtitle("Modularity results for different\n Community detection Algorithms")+
   scale_color_manual(values= ggplotColours(4), guide=FALSE)

setwd(GraphPath)
graph <- read.graph(graphvect[1], format = "graphml")  
bench <- microbenchmark(    fc <-fastgreedy.community(graph),
            wc <-walktrap.community(graph),
            ic <- infomap.community(graph),
            lc <- cluster_louvain(graph),
             times = 30)

bench <-bench %>% rename(Method=expr) %>% 
  mutate(time=(time)/1*10^-9, Method= factor(Method, 
                          levels= levels(Method)[order(levels(Method))],
                          labels = c("Fastgreedy", 
                                   "Infomap",
                                   "Louvain",
                                     "Walktrap" 
                                   )
                          )
         ) 


bench %>% ggplot(., aes(x=Method, y=time, fill= Method)) + geom_boxplot() +theme(legend.position="none") +
  labs(title= "Benchmarking Algorithm run time", y="Run time in seconds") +
  scale_color_manual(values= ggplotColours(4), guide=FALSE)


rm(list = "CommTimes", "graph", "bench")
```

#Assign Cluster Family ID

```{r}

TimePeriods$daypart %>% walk(~{
  print(.x)
  daypart <- .x
  
  print(daypart)
  source(file = file.path(DropboxPath,"SmartMeterThesisCode/SubCode", "PredictionProcess.R"))

  }
)

for(i in 1:nrow(TimePeriods)){
  
  daypart <- TimePeriods$daypart[i]
  StartTimei <- TimePeriods$Start[i]
  EndTimei <- TimePeriods$End[i]
  cutoffi <- TimePeriods$cutoff2[i]
  
  CreateBaseModellingData(daypart, ModelBlock, StartTimei, EndTimei, cutoffi)
  
  source(file = file.path(DropboxPath,"SmartMeterThesisCode/SubCode", "PredictionProcess.R"))

  }

#The plots show that the 24 hour clusters are broad time of day clusters
test <- melt(clusterprofile, id ="time", variable.name = "UniqueID", value.name = "kwh" ) %>%
  left_join(., select(Clustconversion, UniqueID, ClustID), 
            by = "UniqueID") 
test %>% 
  group_by(ClustID, time) %>% 
  summarise(mean= mean(kwh), median=median(kwh), sd=sd(kwh), sum=sum(kwh)) %>% 
  ungroup %>%
  ggplot( aes(x=time, y=mean, group= ClustID, colour= ClustID)) + geom_line() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

test %>% 
  filter(ClustID !="soup") %>%
     ggplot(., aes(x=time,y=kwh, group=UniqueID,colour=ClustID)) +
     geom_line(alpha=0.1)+
     facet_grid(ClustID~.) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 




```


#Predict Transmission

```{r}


PredictedProfiles  <- 1:10 %>% map_df(~{
  
  Fold <- .x
  
  FoldProfiles <- 1:7 %>%
  map_df(~{
    readRDS(file.path(ModelBlock, TimePeriods$daypart[.x], 
                      "PredictedProfiles", 
                      paste0("PredProfiles_Fold",Fold,".rds"))) %>%
      mutate(daypart = TimePeriods$daypart[.x],
        type = case_when(
          grepl("6", daypart) ~"6",
          grepl("12", daypart) ~"12",
          TRUE ~ "WholeDay"
        ),
        Fold = Fold)
    })
})

Pred2 <- PredictedProfiles %>%
  mutate(isReal = ProfileType=="True Profile") %>%
  select(-ClustID,-ProfileType,-type) %>%
  group_by(time, date, daypart, Fold) %>%
  summarise_all(sum, na.rm = T) %>%
   mutate(diffperc = (kwh -TrueValues)/TrueValues,
          day = weekdays(as.Date(date))) %>%
 mutate(day = factor(day, 
                     levels =  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
        type = case_when(
          grepl("6", daypart) ~"6",
          grepl("12", daypart) ~"12",
          TRUE ~ "WholeDay"
        ))

PredictedProfiles %>%
  ggplot(aes(x = time, y = kwh, group = paste(ClustID, date),  colour = ClustID))+ geom_line(alpha = 0.2) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(type~.)

PredictedProfiles %>%
  ggplot(aes(x = time, y = TrueValues, group = paste(ClustID, date),  colour = ClustID))+  geom_line(alpha = 0.2)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

Pred2 %>%
  ggplot(aes(x = time, y = kwh-TrueValues, group = paste(type, date),  colour = type))+ geom_line(alpha = 0.2) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(type~.)

  

TransmissionPerf <- c("6", "12", "WholeDay") %>%
  map_df(~{
    
    Target <- Pred2 %>%
      filter(type == .x)
    
    1:10 %>%
      map_df(~{
        
        Target <- Target %>%
          filter(Fold == .x) 
        
        metrics(Target,  truth = TrueValues, estimate = kwh)%>% 
  mutate(mape =  mape(Target$kwh, Target$TrueValues)) %>%
          mutate(Fold = .x)
        
      }) %>%
      mutate(type = .x)
    
  }) %>%
  mutate(ModelType = "Transmission")
  


DayRes <-unique(Pred2$date) %>% map_df(~{

  Temp <- Pred2 %>%
    filter(date == .x)
  metrics(data = Temp, truth = TrueValues, estimate = kwh) %>% 
  mutate(mape =  mape(Temp$kwh, Temp$TrueValues),
         date = .x,
         day = weekdays(as.Date(.x)))
  
  
})


DayRes %>%
  group_by(day) %>%
  summarise(Mean = mean(mape),
            Median = median(mape)) %>%
  arrange(day)


Preds6 <- PredictedProfiles %>%
  mutate(isReal = ProfileType=="True Profile") %>%
  select(-ClustID,-ProfileType) %>%
  group_by(time, date, daypart) %>%
  summarise_all(sum, na.rm = T) %>%
   mutate(diffperc = (kwh -TrueValues)/TrueValues,
          day = weekdays(as.Date(date))) %>%
 mutate(day = factor(day, 
                     levels =  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) 

DayPartCombos <- expand.grid(unique(Preds6$date), unique(Preds6$daypart))

DayRes6 <- 1:nrow(DayPartCombos) %>% map_df(~{

Temp <- Preds6 %>%
    filter(date == DayPartCombos[.x,1], daypart == DayPartCombos[.x,2])
  metrics(data = Temp, truth = TrueValues, estimate = kwh) %>% 
  mutate(mape =  mape(Temp$kwh, Temp$TrueValues),
         date = .x,
         day = factor(.x, 
                     levels =  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
         daypart = DayPartCombos[.x,2]) 

})

test <- DayRes6 %>%
  group_by(day, daypart) %>%
  summarise(Mean = mean(mape),
            Median = median(mape)) %>%
  arrange(day) %>% ungroup


test %>%
  ggplot(aes(x = day, y = daypart, fill = Mean)) + geom_tile()


Preds6 %>% 
  ggplot(aes(x = daypart, y = TrueValues)) + geom_boxplot()

Preds6 %>% mutate(day = factor(day, 
                     levels =  c("Monday", "Tuesday", "Wednesday", 
                                 "Thursday", "Friday", "Saturday", "Sunday"))) %>%
    ggplot(aes(x = day, y = TrueValues)) + geom_boxplot()


mean(DayRes$mape);median(DayRes$mape)

Pred2 %>%
  ggplot(aes(x = time, y = abs(diffperc), group = date ))+ geom_line(alpha = 0.2) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~day)

Pred2 %>%
  filter(day == "Sunday") %>%
  ggplot(aes(x = time, y = TrueValues, group = date ))+ geom_line(alpha = 0.2) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~day)

Pred2 %>%
  filter(day == "Sunday",
         time == "10:00:00") %>%
  arrange(-TrueValues)

test <- PredictedProfiles %>%
  filter(date == "2012-04-29")
weekdays(as.Date("2012-04-29"))

```


#Create Graphs out of day profiles

```{r}

daypart <- "WholeDay"
Clustconversion <- readRDS(file.path(ModelBlock, daypart ,"Clustconversion.rds"))
 
test <- clusterprofile <- readRDS(file.path(ModelBlock, daypart , "clusterprofile.rds"))  %>%
   melt(., id ="time", variable.name = "UniqueID", value.name = "kwh" ) %>%
  left_join(., Clustconversion %>% select(UniqueID, date)) %>%
  select(-UniqueID) %>%
  group_by(time, date) %>%
  summarise(kwh_sum = sum(kwh, na.rm = T),
            kwh_mean = mean(kwh, na.rm = T),
            isNA  = sum(is.na(kwh)),
            count = n()) %>%
  mutate(DayofWeek = weekdays(date))

test2 <- test %>%
  filter(!is.na(date), isNA==0) %>%
  group_by(date) %>%
  mutate(max = max(kwh_mean),
         min = min(kwh_mean),
         diff = max-min,
         Shape = (kwh_mean-min)/diff)

test %>%
ggplot(aes(x = time, y = kwh_mean, group = date, colour= DayofWeek)) + 
  geom_line(alpha = 0.2) +
  facet_wrap(~DayofWeek)

test2 %>%
ggplot(aes(x = time, y = Shape, group = date, colour= DayofWeek)) + 
  geom_line(alpha = 0.2) +
  facet_wrap(~DayofWeek)

test3 <- test2 %>%
  select(time, date, Shape) %>%
  dcast(time~date, value.var = "Shape", drop=FALSE)


  cutoff <- data.frame(cutoff = c(0,seq(0.1,1,0.05)), edges = NA,nodes=NA)

 cormat <- test3[,2:ncol(test3)] %>%
      as.matrix %>% cor
    diag(cormat) <- 0
    
    cutoff[,-1] <- map_df(cutoff$cutoff ,~{
      logicmat<-cormat>.x
      c(sum(logicmat, na.rm = TRUE),
        sum(rowSums(logicmat,na.rm=TRUE)>0)
      ) %>% 
        t %>% as.data.frame
    })
    
    cutoff %>% 
      mutate(
        NodePerc = nodes/max(nodes),
        EdgePerc = edges/max(edges),
        diff = NodePerc-EdgePerc)

      graph <- createcleangraph2(cormat, cormat > 0.95)

      graph <-detectcomms(graph)
    
 vertices <-data.frame(NodeID = V(graph)$name, 
                          cluster = get.vertex.attribute(graph, "ClusterID"))

  vcount(graph)
 ecount(graph)

  
 plot(graph, vertex.label=NA, vertex.size = 1)

 
orderedheat(cormat, simMat = T, mid = 0.87)


 
```


#Create Day Profile
```{r}
#Create profile for each day
  if(file.exists(file.path(SavedData ,"DayProfile.rds"))){
   
    DayProfile <- readRDS(file.path(SavedData ,"DayProfile.rds"))
    
  }else{
   DayProfile <- list.files(daytimeseries, full.names = TRUE) %>% 
     map_df(~{
       DayProfile <- readRDS(.x)
       
       tibble(DateTime = DayProfile[,1], kwh = rowSums(DayProfile[,-1]))
       }) %>%
    mutate(time =  strftime(DateTime, format="%H:%M:%S", tz = "UTC") ,
           Date = as.Date(DateTime))
  
   saveRDS(DayProfile, file.path(SavedData ,"DayProfile.rds"))
     
  }
```


#Whole day Linear Model
```{r}


AllDates <- unique(DayProfile$Date)
Dayindices <- 1:length(AllDates)
Folds <- split(Dayindices, ceiling(seq_along(Dayindices)/ceiling(length(Dayindices)/10)))

#Create seperate models

LineaModelResults  <- 1:7 %>% map(~{
#.x <- 1
daypart <- TimePeriods$daypart[.x]
StartTime <- TimePeriods$Start[.x]
EndTime <- TimePeriods$End[.x]
print(paste("Calculating Linear model for", daypart))


CrossValidated <- 1:length(Folds) %>% map_df(~{
  TestDates <- AllDates[Folds[[.x]]] 
  TrainDates <- AllDates[!(AllDates %in% TestDates)]
  
  test2 <- DayProfile  %>% filter(as.Date(DateTime) %in% as.Date(TrainDates)) %>%
   mutate(lagtime = lag(kwh, 48)) %>%
   filter(!is.na(lagtime))  %>%
    filter(time >= StartTime, time < EndTime)
  
  test3 <- lm(kwh~lagtime, test2)
  
  test4 <- DayProfile  %>% filter(as.Date(DateTime) %in% as.Date(TestDates)) %>%
  mutate(lagtime = lag(kwh, 48)) %>%
  augment(test3, 
                 newdata =  .) %>%
  filter(!is.na(lagtime)) %>%
  mutate(Fold = .x)

}) %>% 
  mutate(daypart = daypart)

})
names(LineaModelResults) <- TimePeriods$daypart


LinearPerf <- c("6", "12", "WholeDay") %>%
  map_df(~{
    Target <- LineaModelResults[grepl(.x, names(LineaModelResults))]  %>% bind_rows
    
    1:10 %>%
      map_df(~{
        
        Target <- Target %>%
          filter(Fold == .x) 
        
        metrics(Target,  truth = .fitted, estimate = kwh)%>% 
  mutate(mape =  mape(Target$kwh, Target$.fitted)) %>%
          mutate(Fold = .x)
        
      }) %>%
      mutate(type = .x)

}) %>%
  mutate(ModelType = "Linear")



#Shows side by side generic next day prediction for the 3 model types
test <- bind_rows(TransmissionPerf, LinearPerf)

#Linear model clearly outperforms the the transmission model
test %>%
  ggplot(aes(x=type, y = rsq, fill = ModelType))+ geom_boxplot() 

```


Grouping by node similarity

using accuracy to create similarity matrix for smart-meters

```{r}



```

