---
title: "Exploratory code"
author: "Jonathan Bourne"
date: "15 June 2016"
output: html_document
---

Note inlcuding apps is ok atm but this could explode into a large amount of data depending on what other apps appear, I still haven't worked out selectively inlcuding sub folders

includeSyncDropbox("Apps")

includeSyncDropbox("Thesis Data")

Degreeness of the graphs
Betweeness
Life expectancy

```{r Packages}
packages <-c("stringr", "lubridate", "data.table","caret", "xgboost","e1071", "R.utils", "corrplot", "Hmisc", "Amelia", "Matrix", "ff", "ggdendro", "zoo", "networkD3", "igraph","parallel", "magrittr", "ggplot2", "tidyr", "xtable","entropy","dplyr", "microbenchmark", "FeatureHashing")

new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

sapply(packages, library, character.only = TRUE)
rm(list=c("packages",  "new.packages"))
  

isAWS <-(Sys.info()[1]=="Linux")
```

Project Folders
```{r Paths}
#basewd needs to be changed
if(isAWS){
  basewd<- "/home/rstudio/Dropbox/Thesis-Data"
  Figures <- "/home/rstudio/Dropbox/Apps/ShareLaTeX/University College London thesis/Figures"
  Figures <- file.path("/home/rstudio/Dropbox/Apps/ShareLaTeX/University-College-London-thesis/Figures")
    TexTables <- file.path("/home/rstudio/Dropbox/Apps/ShareLaTeX/University-College-London-thesis/Tables")
      functioncode <- file.path(basewd, "SmartMeterThesisCode","Functions")
  Cormats <- "/home/rstudio/Cormats"
GraphPath<-file.path("/home/rstudio", "graphs")
    options(fftempdir = "/home/rstudio")
  } else {
  
  basewd <- "C:/Users/pc1/Dropbox/Thesis-Data"
  Figures <- file.path("C:/Users/pc1/Dropbox/Apps/ShareLaTeX/University-College-London-thesis/Figures")
    TexTables <- file.path("C:/Users/pc1/Dropbox/Apps/ShareLaTeX/University-College-London-thesis/Tables")
  functioncode <- "C:/Users/pc1/Dropbox/Thesis-Data/SmartMeterThesisCode/Functions"
  Cormats <- "C:/Users/pc1/Dropbox/Thesis-Data/Cormats"
  }

SubDataSets <- file.path(basewd, "SubDataSets")
datafile <- file.path(basewd, "TCa1")
daytimeseries <-file.path(basewd,"Cleandata")

  #file.path(basewd,"Cormats")

```

Source functions
```{r Functions}
setwd(functioncode)
sapply(list.files(), source)
```

#Load data

The data is too large to load all at once and perform the filtering operation so the data will be loaded piecewise filtered then all the small pieces will be recombined as searching through the model becomes longer the further down the file it needs to go the file is broken into smaller chunks to keep loading times reasonable.

The data also needs tot be serparatedd into each smart meter type and the time stamps adjusted accordingly. 


```{r Load data from csv}
setwd(datafile)
smartdata <- fread("TrialMonitoringDataHH.csv", drop = c(2,3))
saveRDS(smartdata[1:38400000,], "smartdata1.rds")
saveRDS(smartdata[38400001:76800000,], "smartdata2.rds")
saveRDS(smartdata[76800001:115200000,], "smartdata3.rds")
saveRDS(smartdata[115200000:153956821,], "smartdata4.rds")
rm(smartdata)
```

Load smart meter type, separate into trilliant and logica, then filter to only include the taimes of interest.
```{r Meter Type}
setwd(datafile)
datasource <- read.csv("HalfHourlyDataSource.csv")

files<- list.files()
file.index<- grep(".rds", files)

#Runs in parallel if the code is in aws mode
if(isAWS){
 smartdata<- mclapply(files[file.index], function(n){
    df <- readRDS(n)
    print("Data Loaded")
  filterTime(df,datasource)
  },
  mc.cores = detectCores())
}else{
  smartdata<- lapply(files[file.index], function(n){
    df <- readRDS(n)
    print("Data Loaded")
  filterTime(df,datasource)
  }
)
}
smartdata <- bind_rows(smartdata)
setwd(SubDataSets)
saveRDS(smartdata, "filteredset.rds")
smartdata <-readRDS("filteredset.rds")

```


```{r isnaframe}
#data.table is used as it is better with larger table structures, this may stop being relevant with increased integration with dplyr
smartdata <- dcast(smartdata, Date.Time ~Location.ID, 
              value.var = "Parameter", 
              drop=FALSE)

setwd(SubDataSets)
saveRDS(smartdata,"smartdata.rds")
#smartdata <- readRDS("smartdata.rds")
min(smartdata$Date.Time)
max(smartdata$Date.Time)
max(smartdata$Date.Time)-min(smartdata$Date.Time)


#Create is na frame where 1 is valid and 0 is NA
isnaframe <- 1-is.na(smartdata[,-1])*1
saveRDS(isnaframe, "isnaframe.rds")
#isnaframe <- readRDS("isnaframe.rds")
rm(smartdata)

#How much data is missing as a percentage of total
1-sum(isnaframe)/(ncol(isnaframe)*nrow(isnaframe))

#create the row column ordering for isnaframe
#this takes much longer when it has to write to dropbox

ordering <- createorder(isnaframe, order="both", simMat= FALSE,xblocks=5, yblocks=5, par=TRUE)

saveRDS(ordering, "isnaordering.rds")
#ordering <- readRDS("isnaordering.rds")
```

Pre cleaning unordered
```{r precleaning plot}
test <- orderedheat(isnaframe, order = "none", merge = 5, simMat = FALSE,
                xblocks=10, yblocks=10, mid = 0.5, legend="Percent Valid")
test+     
    labs(x = "Date time",
         y = "Smartmeter ID") +ggtitle("Missing data pre-cleaning")

setwd(Figures)
ggsave("unorderedPrecleaningmissing.pdf")
rm(test)
#smartmeter % complete data

```


Pre cleaning ordered
```{r precleaning ordered}
test <- bigheat(isnaframe[ordering$Roworder,ordering$Colorder],
                merge = 5,mid=0.5, legend="Percent Valid")
test+     
    labs(x = "Date time",
         y = "Smartmeter ID") +ggtitle("Missing data pre-cleaning")
ggsave("Precleaningmissing.pdf")
rm(test)

```



highlighting smart meter groups in the correct time order. 
As there are two clear groups of smartmeters and a group of smart meters that have not delivered very good quality information, it is important to look at how the clusters behave in normal time
```{r extract smartmeters}

findbreak<- t(isnaframe[ordering$Roworder, ordering$Colorder[300:6000]]) %>% as.data.frame %>%
  mutate(rowsum = rowSums(.), 
         diff= lag(rowsum,1), 
         rowID= 1:nrow(.), 
         rM=(rowsum + lag(rowsum)+lead(rowsum))/3) %>% 
  select(rowsum, diff, rowID, rM)

ggplot(findbreak, aes(x = rowID, y = rowsum)) + geom_line() +
  ggtitle("Identifying break points in the smartmeter clusters") +
  labs(x="Cluster ordered MeterIDs", y = "Number of Valid data points")
ggsave("breakpoints.pdf")
#break point at groups at 1:2380 and 2381:4530 add list of smart meters in the appendix along with time periods
```


Lower Cluster shown in chronological time 
```{r plot valid meters}

#ensures aggregation happens correctly
lowerclustID <- 300:6000 #the smart meters to select

test <- bigheat(isnaframe[,ordering$Colorder[lowerclustID]],
                merge = 5,mid=0.5, legend="Percent Valid")
test+     
    labs(x = "Date time",
         y = "Smartmeter ID") +ggtitle("Missing data pre-cleaning")

ggsave("LowerPrecleaningmissing.pdf")

```


#Removing highly missing data

Now the data is broken into two clusters of smart meters the time componant can be filtered to leave  high quality data set.



Cleaning the cluster
```{r cleaningthedata}
setwd(SubDataSets)
#makes a matrix where 1 means there is data and 0 means NA
lowerclust <- isnaframe[, ordering$Colorder[lowerclustID]]%>% as.data.frame
saveRDS(lowerclust, "lowerclust.rds")
#lowerclust <- readRDS("lowerclust.rds")
lowertimepercs <- rowSums(lowerclust)/ncol(lowerclust)

setwd(Figures)


#create a data frame showing how many time periods have more than x% values
nonmissing <- data.frame(cutoff = seq(0.1,1,0.01), TimePeriods =NA, SmartMeters = NA)

nonmissing$TimePeriods <- sapply(nonmissing$cutoff ,function(n) {
  sum(lowertimepercs>n, na.rm = TRUE)
  })

ggplot(nonmissing, aes(x= cutoff, y= TimePeriods)) + geom_line() +ggtitle("Number of Time Periods that have at least \nthe percentage of valid data indicated by the cut off") +xlab("Cut Off") +ylab("Number of Valid Time Periods")
ggsave("NAtimeperiodslowerclust.pdf")


#Remove Time periods with less than 90% valid data
lowerclust <- lowerclust[lowertimepercs>0.9,]

lowermeterpercs <- colSums(lowerclust)/nrow(lowerclust)

nonmissing$SmartMeters <- sapply(nonmissing$cutoff ,function(n) {
  sum(lowermeterpercs>n, na.rm = TRUE)
  })

ggplot(nonmissing, aes(x= cutoff, y= SmartMeters)) + geom_line() +ggtitle("Number smart meters that have at least \nthe percentage of valid data indicated by the cut off") +xlab("Cut Off") +ylab("Number of Valid smart meters")
ggsave("NAsmartmeters.pdf")

#filter the meters
lowerclust <- lowerclust[,lowermeterpercs >0.99]
totalmeters <- sum(lowermeterpercs >0.99)

#How much data is missing as a percentage of total post cleaning
sum((lowerclust))/(ncol(lowerclust)*nrow(lowerclust))
rm(isnaframe)
rm(lowerclust)

setwd(SubDataSets)
smartdata <- readRDS("smartdata.rds")

#Check how many days are missing to have contiguous days from start to finish in the block

datevect <- as.Date(smartdata$Date.Time)[lowertimepercs>0.9] %>% unique
alldays <-seq(from=min(datevect) , to=max(datevect),by = "day") 
MissingDays <-alldays[!(alldays %in% unique(datevect))] 

#there are only three days missing for a full house make a vector to inlcude them as well
MissingDays <-as.Date(smartdata$Date.Time) %in% MissingDays

cleandata <-smartdata[,c(1,(1+ordering$Colorder[lowerclustID]))]
size <- ncol(cleandata)*nrow(cleandata)
cleandata <- cleandata[(lowertimepercs>0.9|MissingDays), c(TRUE,lowermeterpercs >0.99)]



ncol(cleandata)*nrow(cleandata)/size #amount of remaingin data

saveRDS(cleandata, "cleandata.rds")
#cleandata <-readRDS("cleandata.rds")
```

The result of cleaning both the cluster is that only minor smart meter removal specific removal needs to take place after the time periods have been cleaned up. This suggests that within the clusters data quality is strongly related related to time period and not to smart meter.

#Exploring the data

How many days are full days?
```{r}
fulldays <- cleandata %>% group_by(date(Date.Time)) %>% summarise(total = n()) %>%
  rename(Date.Time = `date(Date.Time)`)
table(fulldays$total)

```


how many days have date 1 day before and 7 days before?
```{r}
weekdiff <- fulldays$Date.Time -ddays(7)
sum(weekdiff %in% fulldays$Date.Time) #239 days

weekdiff <- fulldays$Date.Time -ddays(1)
sum(weekdiff %in% fulldays$Date.Time) #239 days

sum(is.na(cleandata))

sum(is.na(cleandata))/size

#The missingness of the days that have been included even though they don't make the cut.
day1<- cleandata %>% filter(as.Date(Date.Time)==ymd("2011-06-29")) %>%is.na %>% sum
1-day1/(12*5261)

day1<- cleandata %>% filter(as.Date(Date.Time)==ymd("2011-10-31")) %>%is.na %>% sum
1-day1/(12*5261)
day1<- cleandata %>% filter(as.Date(Date.Time)==ymd("2011-12-12")) %>%is.na %>% sum
1-day1/(12*5261)

```



Fill in missing values by day time average, then average by day using a three time period window
```{r fill in missing}
setwd(SubDataSets)
cleandata <- readRDS("cleandata.rds")
#add in missing row for day 177
missingrow <-matrix(NA, nrow=1,ncol=ncol(cleandata)) %>% as.data.frame %>%
  mutate_all(funs(as.numeric))
names(missingrow) <- names(cleandata)

missingrow <- missingrow  %>% mutate(Date.Time= as.POSIXct("2011-10-25 18:00:00", tz="Europe/London") + minutes(30))

cleandata <- cleandata %>% bind_rows(., missingrow) %>% 
  arrange(Date.Time)

#make a data frame of average day hour in values
dayhourmin <- paste(wday(cleandata$Date.Time),
                    hour(cleandata$Date.Time),
                    minute(cleandata$Date.Time),
                    sep=":")

meanvals <- cleandata[,-1] %>%
  mutate(time.day = dayhourmin) %>% group_by(time.day) %>%
  summarise_each(funs(mean(., na.rm=TRUE))) %>%ungroup

navect <- cleandata %>% is.na %>% which(., arr.ind=T)

NACols <- unique(navect[,2] )

for(i in 1:length(NACols)){
colID <-NACols[i]
rowIDs <- navect[navect[,2]==colID,1]

RowsFromMeanVals<- match(dayhourmin[rowIDs],meanvals$time.day)

cleandata[rowIDs,colID] <- meanvals[RowsFromMeanVals,colID] %>%unlist
if((i%%100)==0){print(i)}  
}

#check there are no Na values
cleandata %>% is.na %>% sum
saveRDS(cleandata, file="cleandatafilled.rds")
#cleandata <- readRDS("cleandatafilled.rds")


rm(list= c("NACols","dayhourmin", "i","RowsFromMeanVals", "navect", "meanvals", "colID", "rowIDs", "missingrow"))



```


```{r Internal Cor}
set.seed(1238)
NodeIDs <- sample(1:5260, 50)+1
IntCor <- cleandata[,c(1,NodeIDs)] %>%
  mutate(Time = paste(hour(cleandata$Date.Time),
                    minute(cleandata$Date.Time),
                    sep=":"),
         Date = as.Date(Date.Time)
)%>%  gather(key = NodeID, value = kwh, 
             -Date.Time, 
             -Time,
             -Date) %>% select(-Date.Time) %>%
  spread(key= Time, value=kwh)

IntCorList <- mclapply(unique(IntCor$NodeID), function(n){
  
  IntCor %>% filter(NodeID ==n) %>% select(-NodeID, -Date) %>%
    t %>%
    cor
  },
mc.cores=detectCores())

#visualise the corellation matrix of the first smart mater
IntCorVis <-orderedheat(IntCorList[[2]], order="both", simMat = TRUE, merge = 1, mid = 0)
IntCorVis

IntCorVis2 <-orderedheat(abs(IntCorList[[1]]), order="both", simMat = TRUE, merge = 1)
IntCorVis2

  MeanAbsCor <- sapply(1:50, function(n) mean(abs(IntCorList[[n]]))) %>% data.frame(value=., NodeID= NodeIDs)  
  
  ggplot(MeanAbsCor, aes(x= value)) + geom_density(fill="steelblue", alpha =0.7) +
    labs(title= "Mean absolute correlation for 50 nodes with themselves", 
         x= "Mean absolute Corellation" )
  setwd(file.path(Figures, "Results"))    
  ggsave("MeanAbsCorr.pdf")
  
  
  #Distribution of the data
  
cleandata %>% gather(. , key=SMartID, value=kWh, -Date.Time) %>%
  ggplot(., aes(x=kWh)) + geom_density(fill="steelblue", alpha =0.7) + ggtitle("Distribution of energy consumption")
ggsave("energydensity.pdf")
  
cleandata %>% gather(. , key=SMartID, value=kWh, -Date.Time) %>%
  ggplot(., aes(x=log10(kWh))) + geom_density(fill="steelblue", alpha =0.7) + ggtitle("Distribution of energy consumption")
  setwd(file.path(Figures, "Appendix")) 
ggsave("logenergydensity.pdf")


```


Save each day as a file for making into graphs
```{r savedaydata}

datevect <- cleandata$Date.Time %>%as.Date() %>% unique
# i<-177
# test <- filter(cleandata, as.Date(Date.Time)==datevect[i])
# filter(cleandata, as.Date(Date.Time)==datevect[i]) %>%
#     saveRDS(., paste("date_", datevect[i] ,sep=""))

setwd(daytimeseries)
for (i in 1:length(datevect)){
  filter(cleandata, as.Date(Date.Time)==datevect[i]) %>%
    saveRDS(., paste("date_", datevect[i] ,sep=""))
   print(i) 
    
}

rm(cleandata)

#check that all the day data has 0 NA and all nodes
setwd(daytimeseries)
files<-list.files()
filecheck <- lapply(files, function(n){
  print(n)
  x <- readRDS(n)
  x<- data.frame(date= sub("date_","",n), 
             nodes=ncol(x)-1,NAs=sum(is.na(x)))
  return(x)
}) %>% bind_rows()

summary(filecheck)

rm(filecheck)
```



Create distance matrix for each day
```{r distmatday}
CreateAllCormats(daytimeseries, Cormats, parallel.mode = TRUE)
CreateAllGraphs(Cormats, GraphPath, 
                cutoff = 0.7, parallel.mode = TRUE)

#Plot an example corellation matrix
setwd(Cormats)
df <- readRDS(list.files()[101])
df[is.na(df)]<-0
orderedheat(df, 
            simMat = TRUE, 
            legend= "Correlation score", mid=0)+ggtitle("Day 09-8-2011 corellation plot, ordered by similarity") +xlab("nodes")+ylab("nodes")
setwd(file.path(Figures,"Method"))
ggsave("day100Corplot.pdf")
rm(df)
```

Look at the number of edges for a large collection of days
```{r totaledges}

setwd(Cormats)  
cutoff <- data.frame(cutoff = seq(0.1,1,0.05), edges = NA,nodes=NA)

files <-list.files()

x <-lapply(1:length(files), function(y){
    distmat <-readRDS(files[y])
    print(paste(y,"of", length(files)))
    cutoffx <-cutoff  

        cutoffx[,-1] <- lapply(cutoffx$cutoff ,function(n) {
      logicmat<-distmat>n  
      c(sum(logicmat, na.rm = TRUE),
        sum(rowSums(logicmat,na.rm=TRUE)>0) %>%as.data.frame
      )
        }) %>% bind_rows
          cutoffx}

    )
#setwd(SubDataSets)
#saveRDS(x, "alldayscutoffs.rds")
#x <- readRDS("alldayscutoffs.rds")

edges <- data.frame(cutoff=cutoff[,1])

for (i in 1:length(x)){
edges  <-cbind(edges,x[[i]][,2])
  
} %>% as.data.frame

names(edges)[-1] <- make.names(1:(ncol(edges)-1))
edgestats <- edges  %>% gather(., key=day, value= edges,-cutoff) %>% 
  group_by(cutoff) %>%
  summarise(mean = mean(edges), median =median(edges),stdev = sd(edges)) %>%
  mutate(upper=mean+stdev, lower= mean-stdev) %>% ungroup %>% 
  gather(., key=Average, value, -cutoff, -(stdev:lower))

setwd(file.path(Figures,"Method"))

edgestats %>% ggplot(., aes(x=cutoff)) +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill="grey70")+
  geom_line(aes( y =value, color= Average))+
  ggtitle("Mean and median edges with standard deviation")
ggsave("Meanedges.pdf")


nodes <- data.frame(cutoff=cutoff[,1])

for (i in 1:length(x)){
nodes  <-cbind(nodes,x[[i]][,3])
  
} %>% as.data.frame()

names(nodes)[-1] <- make.names(1:(ncol(nodes)-1))
nodestats <- nodes  %>% gather(., key=day, value= nodes,-cutoff) %>% 
  group_by(cutoff) %>%
  summarise(mean = mean(nodes), median =median(nodes),stdev = sd(nodes)) %>%
  mutate(upper=mean+stdev, lower= mean-stdev) %>% ungroup %>% 
  gather(., key=Average, value, -cutoff, -(stdev:lower))

nodestats %>% filter(Average== "mean") %>% ggplot(., aes(x=cutoff)) +
  geom_line(aes( y =value), colour="black")+
  ggtitle("Mean number of Nodes and variance")
ggsave("Meannodes.pdf")


#make plot that combines edges and nodes as a percent

edgestats2 <- edgestats %>% mutate(Average= paste("Edge", Average),
                                   upper=upper/(5209*5208/2),
                                   lower = lower/(5209*5208/2),
                                   value = value/(5209*5208/2))

nodestats2 <- nodestats %>% mutate(Average= paste("Node", Average),
                                   upper=upper/5209,
                                   lower = lower/5209,
                                   value = value/5209)

bind_rows(edgestats2,nodestats2) %>% filter(Average !="Node median") %>%
  ggplot(., aes(x=cutoff)) +
  geom_ribbon(aes(ymin=lower, ymax=upper, colour = Average), fill="grey70")+
  geom_line(aes( y =value, color= Average))+
  ggtitle("Mean and median edges with standard deviation")
ggsave("Edgenodeperc.pdf")


nodestats2 %>% select(cutoff,Average ,value) %>% 
  mutate(value= value-edgestats2$value) %>% 
  filter(Average=="Node mean") %>%
  ggplot(., aes(x= cutoff, y=value)) +
  geom_line()+ ggtitle("Difference between Remaining Node and Edge means")
  ggsave("EdgeNodeDiff.pdf")

```

The results of the above analysis suggest that a 0.7 cutoff will be chosen.

```{r ER vs dataset}

#create a dataframe with the number of required edges and the space for the iterations
DataVsER <- edgestats %>% filter(Average=="mean")%>%
  select(cutoff, value)%>% rename(edges= value) %>% 
  cbind(., data.frame(matrix(data=NA,nrow=nrow(.), ncol = 20)))

#generate 20*19 graphs
for(n in 1:20){
for(i in 1:nrow(DataVsER)){
g <- sample_gnm(totalmeters, DataVsER$edges[i])
DataVsER[i,2+n] <- totalmeters-(degree(g)==0) %>% sum
print(paste("graph", i, "of iteration", n))
}

}
rm(g)
setwd(SubDataSets)
saveRDS(DataVsER, "DataVsER.rds")
#DataVsER<- readRDS("DataVsER.rds")
#find the average number of valid rows
pairtTtest <-bind_rows(data.frame(nodes= DataVsER[,-c(1:2)] %>%
                       rowMeans(),cutoff=seq(0.1,1,0.05), type="ER"),
          nodestats %>% filter(Average=="mean") %>%
            select(value) %>%rename(nodes=value) %>%
            mutate(cutoff=seq(0.1,1,0.05), type="Data")
          )

          
#there is zero std dev on all rows apart from row 18 which has a very small standard deviation
DataVsER[18,-c(1:2)] %>%sd


#perform a paired T-test to see if there is a significant difference in between the number of isolated nodes in an ER graph and the data set.
t.test(DataVsER[,-c(1:2)] %>%
                       rowMeans(), (nodestats %>% filter(Average=="mean"))$value, paired = TRUE)


#There is a bit of a bug, but it shows that there is a significant difference between the two graph types, check for specific n values e.g n<-17
sapply(1:20, function(n) {
  t.test(DataVsER[n,-c(1:2)],nodes[n,sample(1:246,20)+1])$p.value
  })

#conclusion is that there is a clear and significant different between the number of isolated nodes in the data we are using and an ER graph

ggplot(pairtTtest, aes(x=cutoff, y=nodes, colour=type)) +geom_line() +
  ggtitle("Difference between ER and smartmeter non-isolated nodes")
setwd(file.path(Figures,"Method"))
ggsave("DataVsER.pdf")
```


This section shows why I chose the method I did
```{r Benchmarking Community Detection}

#why  doesn't cluster detection work on corellation plots?
setwd(Cormats)
n <- 10
filevect <- sample(list.files(),30)

    graph2 <- readRDS(filevect[n])
    graph2 <-createcleangraph2(graph2, graph2>0.7 )

    fc <-fastgreedy.community(graph2)
    wc <-walktrap.community(graph2)
    ic <- infomap.community(graph2)
    lc <- cluster_louvain(graph2)
    
    modularity(fc)
    modularity(wc)
    modularity(ic)
    modularity(lc)
    
    membership(wc) %>% unique 
    membership(fc) %>% unique 
    membership(ic) %>% unique 
  
setwd(GraphPath)
graphvect <- sample(list.files(),30)

CommTimes<-mclapply(1:30, function(n){
    graph <- read.graph(graphvect[n], format = "graphml")  

    fc <-fastgreedy.community(graph)
    wc <-walktrap.community(graph)
    ic <- infomap.community(graph)
    lc <- cluster_louvain(graph)
    mod<- data.frame( 
      Fastgreedy= modularity(fc),
      Walktrap= modularity(wc),
      Infomap = modularity(ic),
      Louvain =modularity(lc))
    },
    mc.cores=detectCores()

)%>% bind_rows()  
  
CommTimes <- CommTimes %>% gather(., key= Method, value= Modularity) %>% 
  mutate(Method= factor(Method, 
                          labels = c("Fastgreedy", 
                                   "Infomap",
                                   "Louvain",
                                     "Walktrap" 
                                   ))
  )

CommTimes %>% ggplot(., aes(x=Method, y=Modularity, fill= Method)) + geom_boxplot() +theme(legend.position="none") +
  ggtitle("Modularity results for different\n Community detection Algorithms")+
   scale_color_manual(values= ggplotColours(4), guide=FALSE)
setwd(file.path(Figures, "Results"))
ggsave("CommModComp.pdf")

setwd(GraphPath)
graph <- read.graph(graphvect[1], format = "graphml")  
bench <- microbenchmark(    fc <-fastgreedy.community(graph),
            wc <-walktrap.community(graph),
            ic <- infomap.community(graph),
            lc <- cluster_louvain(graph),
             times = 30)

bench <-bench %>% rename(Method=expr) %>% 
  mutate(time=(time)/1*10^-9, Method= factor(Method, 
                          levels= levels(Method)[order(levels(Method))],
                          labels = c("Fastgreedy", 
                                   "Infomap",
                                   "Louvain",
                                     "Walktrap" 
                                   )
                          )
         ) 
setwd(SubDataSets)
saveRDS(bench, "TimeBenchmark.rds")
#bench <-readRDS("TimeBenchmark.rds")


bench %>% ggplot(., aes(x=Method, y=time, fill= Method)) + geom_boxplot() +theme(legend.position="none") +
  labs(title= "Benchmarking Algorithm run time", y="Run time in seconds") +
  scale_color_manual(values= ggplotColours(4), guide=FALSE)
setwd(file.path(Figures, "Results"))
ggsave("AlgorithmTimes.pdf")

rm(list = "CommTimes", "graph", "bench")

```



#Creating Graphs

Create graph
NA's caused an automatic link and so all NA's were replaced with 0's
```{r singlegraph}

setwd(daytimeseries)
setwd(Cormats)
filename <- list.files()[177]#day 101 is the cool ring graph when cut off is .92

graph2 <- readRDS(filename)
graph2 <-createcleangraph2(graph2, graph2>0.7 )
graph2 <-detectcomms(graph2)

setwd(SubDataSets)
write.graph(graph2, "day100-92.graphml", format = "graphml")
get.vertex.attribute(graph2, "ClusterID", index=V(graph2)) %>% unique()

setwd(GraphPath)
write.graph(graph2, "date_2011-10-25.graphml", format = "graphml")


graph2 <- readRDS(filename)
graph2 <-createcleangraph2(graph2, graph2>0.7 )
fc <-fastgreedy.community(graph2)
wc <-walktrap.community(graph2)
sc <- spinglass.community(graph2)
ic <- infomap.community(graph2)
membership(wc) %>% unique 
membership(fc) %>% unique 
membership(ic) %>% unique 


modularity(fc)
modularity(wc)
modularity(ic)
#Is it worth comparing the structual similarity of the betweenes of the graphs by doing a corellation matrix of the graph betweeness for all graphs?
betweengraph2 <- betweenness(graph2)

```


Make and save all graphs
```{r Allgraphs}

#load day make graph save graph file as an graphml
CreateAllGraphs(Cormats, GraphPath, 
                cutoff = 0.7, parallel.mode = TRUE)


#check graph size
setwd(GraphPath)
files<-list.files()
nodecount <-sapply(files, function(n){
      print(n)
      graph <- read.graph(files[1], format="graphml")
    vcount(graph)
  }
)

#there appears to be 5260 nodes in every day when sweep is used for graph creation. When sweep is off then there are 5260
summary(nodecount)
```


Create a cluster Makeup vector
Take the nodes that make up each vector and construct a vector that includes all nodes and ones where a node is included in the cluster
```{r nodeclustlist}

#Load each graph file and create a list of clust vector matrices
setwd(GraphPath)
files <- list.files()

#The Mclapply version kept crashing so the part is done in single

nodeclustlist <-mclapply(files, function(n){
  graph <- read_graph(n, format="graphml")
  vertices <-data.frame(NodeID= V(graph)$name, 
                      cluster=
                        get.vertex.attribute(graph, "ClusterID")) %>%
                        mutate(yesno=1)%>%
  spread(., key= cluster, value=yesno) %>% 
  mutate(NodeID= as.character(NodeID))
  print(paste("file:", n," date:",  grep(n, files)))
  vertices
}, mc.cores = detectCores()
)


names(nodeclustlist)<-sub(".graphml", "",files) %>%sub("date_","",.) %>%ymd

setwd(SubDataSets)
saveRDS(nodeclustlist, "nodeclustlist.rds")
#nodeclustlist <- readRDS("nodeclustlist.rds")


#does this need to be tested to make sure the nodes are al in the same order?
test <- lapply(nodeclustlist, function(n) n[,1])%>% bind_cols() %>%
  t %>% as.data.frame %>% mutate_all(funs(as.factor))

#the results show that each row only has 1 unique variable which means the nodes are all in the same order
test %>% sapply(., function(n) unique(n) %>% length) %>% summary

#make a dataframe that contains all nodes
setwd(daytimeseries)

#as all the nodes are inlcuded any day can be used
NodeClust<- readRDS(list.files()[1]) %>%names() 

NodeClust<- data.frame(NodeID=as.character(NodeClust[-1])%>% 
  make.names) %>%mutate(NodeID = as.character(NodeID)) %>% arrange(NodeID)

#check NodeClust and Nodeclustlist are in the same order
all.equal(NodeClust$NodeID, nodeclustlist$`2011-05-01`$NodeID )
#they are because NodeClust List has been arranged in the correct order

testmat <- lapply(nodeclustlist, function(n) n[,-1]) %>% bind_cols()
testmat[is.na(testmat)] <- 0
(testmat %>% colSums())[c(4293, 6066)]

#This messes up the order 
# #combine all cluster matrices into a single cluster matrix
# testmat <- Reduce(function(...) merge(..., 
#                                       by="NodeID", 
#                                       all=TRUE), 
#                   nodeclustlist)

NodeClust<-bind_cols(NodeClust, testmat)
rm(testmat)

#why is nodeclust list not doing what I would expect?
testmat2 <-readRDS(list.files()[1]) %>%names()  %>% data.frame(NodeID=as.character(.[-1])%>% 
  make.names) %>%mutate(NodeID = as.character(NodeID))

#names(NodeClust)[-1]<-make.names(1:ncol(NodeClust[,-1])) #give them all unique names


```


Make a conversion table for original cluster names to new cluster names
also 

```{r clusterconversion}
setwd(SubDataSets)
nodeclustlist <- readRDS("nodeclustlist.rds")

#get day order from the nodeclustlist object
dayorder <- names(nodeclustlist)%>%ymd

Clustconversion <-lapply(1:length(nodeclustlist), function(n){
      day <-data.frame(oldClustID=nodeclustlist[[n]][,-1] %>%
                   names %>% as.character, stringsAsFactors = FALSE) %>%
    mutate(day=n, date=dayorder[n],
           NodesInCluster=colSums(nodeclustlist[[n]][,-1], na.rm=TRUE),
           IntraDayRank=rank(-(NodesInCluster),ties.method="random"))
      day
      } 
    ) %>%bind_rows %>% mutate(UniqueID = 1:nrow(.)%>%make.names)

rm(dayorder)

clustdaycount <- Clustconversion %>% group_by(day) %>%
  summarise(counts= n()) %>% ungroup %>% 
  mutate(cumsum = cumsum(counts), startrow= cumsum(c(1,counts))[-(nrow(.)+1)], endrow = cumsum)


#apply the unique names to the NodeClust frame, this could have perhaps been done in a better way... but I didn't so whatevs
names(NodeClust)[-1] <-Clustconversion$UniqueID

setwd(SubDataSets)
saveRDS(NodeClust, "NodeClust.rds")
#NodeClust<-readRDS("NodeClust.rds")


```


```{r Load clustering}

setwd(daytimeseries)
dates <- list.files() %>%sub("date_", "",.)

#Makes a load profile for every single cluster represented in the clusterconversion dataframe... take time to calculate


allprofiles<-mclapply(dates, function(n){
  daydata <- CreateLoadProfile(n)
  variable <- "mean" #the metric to use
  print(n)

    daydata2 <- daydata %>% 
    select_("ClusterID", "time", variable) %>%
    spread_(., key="ClusterID", value=variable) %>% 
    mutate(time= format(.$time, format="%H:%M:%S"))
  return(daydata2)
  }, mc.cores = detectCores())

#inspect a random cluster profile
test <- allprofiles[[177]] %>% gather(., key= clusterID, value=kwh, -time) 
ggplot(test, aes(x=time, y=kwh,group= clusterID, colour=clusterID)) +geom_line()

#this second bit of code is because I got worried the nodes weren't being linked in the right order
#creates a loadprofile data frame of all the uniqely named clusters across all time periods
clusterprofile <- lapply(1:length(allprofiles), function(n){
    dayID <-n
    print(n)
    x <-allprofiles[[dayID]]
    z <-Clustconversion %>% filter(day==dayID)
    names(x)[match(z$oldClustID,names(x))]<-z$UniqueID

    return(x)
  }
) 


clusterprofile2<- clusterprofile
clusterprofile <- clusterprofile[[1]] %>% select(time)

for (i in 1:length(clusterprofile2)) {
  print(i)
  clusterprofile <- left_join(clusterprofile, clusterprofile2[[i]], by="time")   
}

rm(clusterprofile2)

setwd(SubDataSets)
saveRDS(clusterprofile,"clusterprofile.rds" )
#clusterprofile<-readRDS("clusterprofile.rds")

ClustComms <- cor(clusterprofile[,-1])

# the below vector shows which of the columns have smart meters with zero varience, they will return NA's in the corellation step
nearZeroVar(clusterprofile)

ClustComms[is.na(ClustComms)]<-0 #remove NA's for clustering
saveRDS(ClustComms,"ClustComms.rds")
#ClustComms <-readRDS("ClustComms.rds")

orderedheat(ClustComms, merge=5, mid=0, simMat=TRUE, legend = "corellation") + ggtitle("Cluster Corellation using Average Load Profile") +xlab("Cluster") +ylab("Clusters")
setwd(file.path(Figures,"Results"))


LargeClusts <- filter(Clustconversion, NodesInCluster>50)$UniqueID %>%
  match(.,colnames(ClustComms))


ClustComms2 <- ClustComms[LargeClusts,LargeClusts]

orderedheat(ClustComms2, merge=1, mid=0, simMat=TRUE, legend = "correlation") + ggtitle("Cluster Corellation using Average Load Profile") +xlab("Clusters") +ylab("Clusters")+
  theme(axis.text.x  = element_blank(), axis.text.y = element_blank()) 
setwd(file.path(Figures,"Results"))
#saves in png becuase otherwise the fie is 6mb
ggsave("ClusterCorLoadLarge.png")

ClustComms2[is.na(ClustComms2)]<-0
Clustergraph <- createcleangraph2(ClustComms2, ClustComms2 >0.85)
      print("Graph created")
      Clustergraph <-detectcomms(Clustergraph)
      print("Clusters detected")

      setwd(SubDataSets)
write.graph(Clustergraph, "ClustComms.graphml", format="graphml")
#Clustergraph <- read.graph("ClustComms.graphml", format="graphml")


fc <- fastgreedy.community(Clustergraph)
      wc<-walktrap.community(Clustergraph)
      ic <- infomap.community(Clustergraph)
      modularity(fc)
      modularity(wc)
      modularity(ic)

      ecount(Clustergraph)
  vcount(Clustergraph)
```



```{r Conversion ClustID}

UniqueClusters<- data.frame(UniqueID= V(Clustergraph)$name ,
                      ClustID= get.vertex.attribute(Clustergraph, name = "ClusterID"),
                      stringsAsFactors =FALSE)

Clustconversion <- Clustconversion%>% 
  left_join(.,UniqueClusters, by="UniqueID") %>%
  mutate(ClustID= ifelse(is.na(ClustID),"soup",ClustID))

#remove clusters that have less than 1% of node volumne
LargeClusts <- Clustconversion %>% group_by(ClustID) %>% 
  summarise(TotalNodes= sum(NodesInCluster), counts = n()) %>%
  ungroup %>% 
  mutate(PercVol = TotalNodes/sum(TotalNodes), 
         ClustID2 = ifelse(PercVol<0.01, "soup", ClustID))

Clustconversion <- LargeClusts %>% select(ClustID, ClustID2) %>%
  left_join(Clustconversion, LargeClusts, by="ClustID") %>%
  select(-ClustID) %>%rename(ClustID=ClustID2) %>% 
  arrange(sub("X", "",UniqueID) %>% as.numeric)

saveRDS(Clustconversion,"Clustconversion.rds")
rm(list=c("UniqueClusters", "LargeClusts"))


    Clustsperday<-  Clustconversion %>% group_by(ClustID, date) %>% 
        summarise(Occurances=n())%>% ungroup %>%
        spread(., date,value=Occurances, fill=0)

    orderedheat((t(Clustsperday[,-1])>0)*1, merge=1)      

    V(Clustergraph)$name

    #rename the cluster of clusters graph to have the correct cluster names
    graph <- set.vertex.attribute(Clustergraph, 
             name = "ClusterID", 
             index = V(Clustergraph), 
             value =Clustconversion$ClustID[Clustconversion$NodesInCluster>50])

    setwd(SubDataSets)
    write.graph(graph, "ClustComms.graphml", format="graphml")
#Clustergraph <- read.graph("ClustComms.graphml", format="graphml")
    
  get.vertex.attribute(graph, "ClusterID")
    
```

Naming and plotting clusters

This chunk dosn't make perfect sense interms of  pure analysis but it is conveniant to name the cluster and plot them at the same time although obviously it the clusters need to be plotted before they can be named 
```{r Cluster Profile Plots}

#When changing from walk trap to louvain this code needed to be replaced. However the difference between walktrap and louvain is interesting so may need comment.
# namedclusts <- data.frame(ClustIDs = unique(Clustconversion$ClustID),
#                           ClustNames = c("1600","soup", "1730","1900","1830", "2100", "2000","1700" , "1800", "2130"),
#                           stringsAsFactors = FALSE)
# 

namedclusts <- data.frame(ClustIDs = unique(Clustconversion$ClustID),
                          ClustNames =  c("1800","soup", "2000","1900","1830", "1730", "1630","1700" ),
                          stringsAsFactors = FALSE)



Clustconversion <-Clustconversion %>% mutate(ClustID =  namedclusts$ClustNames[match(Clustconversion$ClustID, namedclusts$ClustIDs)])


#save the clusterconversion table with the named clusters
setwd(SubDataSets)
saveRDS(Clustconversion, "Clustconversion.rds")

test <- clusterprofile %>% 
  gather(., key=UniqueID, value= kwh,-time) %>%
  left_join(., select(Clustconversion, UniqueID, ClustID), 
            by = "UniqueID") %>% group_by(ClustID, time) %>% 
  summarise(mean= mean(kwh), median=median(kwh), sd=sd(kwh), sum=sum(kwh)) %>% ungroup %>% mutate(time =gsub('.{3}$', '', time))

#weights the kwh 
sumkwh <-select(clusterprofile,-time) %>% as.matrix

sumkwh <- (t(sumkwh) * Clustconversion$NodesInCluster) %>% t %>% as.data.frame %>%
  bind_cols(select(clusterprofile,time), .) %>% gather(., key=UniqueID, value= kwh,-time) %>%
  left_join(., select(Clustconversion, UniqueID, ClustID), 
            by = "UniqueID") %>% group_by(ClustID, time) %>% 
  summarise(sum=sum(kwh)) %>% ungroup %>% mutate(time =gsub('.{3}$', '', time))


setwd(file.path(Figures, "Results"))
ggplot(test, aes(x=time, y=mean, group= ClustID, colour= ClustID)) + geom_line() + ggtitle("Mean Cluster Profiles") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("ProfileMean.pdf")
ggplot(test, aes(x=time, y=median, group= ClustID, colour= ClustID)) + geom_line() + ggtitle("Median Cluster Profiles")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("ProfileMedian.pdf")
ggplot(test, aes(x=time, y=sd, group= ClustID, colour= ClustID)) + geom_line() + ggtitle("Sd Cluster Profiles")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("ProfileSd.pdf")
ggplot(sumkwh, aes(x=time, y=sum, group= ClustID, colour= ClustID)) + geom_line() + ggtitle("Sum Cluster Profiles")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("ProfileSum.pdf")


test2 <- clusterprofile %>% 
  gather(., key=UniqueID, value= kwh,-time) %>%
  left_join(., select(Clustconversion, UniqueID, ClustID), by = "UniqueID") %>% mutate(time =gsub('.{3}$', '', time))

test2 %>% 
  filter(ClustID=="1630"|ClustID=="1830"|ClustID=="2000") %>%
     ggplot(., aes(x=time,y=kwh, group=UniqueID,colour=(ClustID))) +
     geom_line(alpha=0.1)+
     facet_grid(ClustID~.)
     ggtitle("Load Profiles of all Clusters within a family") +
     scale_color_manual(values= ggplotColours(8)[c(1,4,8)], guide=FALSE)
ggsave("ClusterFamilySample.pdf")

setwd(file.path(Figures,"Appendix"))
  test2 %>%
     ggplot(., aes(x=time,y=kwh, group=UniqueID,colour=(ClustID))) +
     geom_line(alpha=0.1)+
     facet_wrap(~ClustID)+
     ggtitle("Exploration of the load profiles in each cluster family")+
    theme(legend.position="none", 
          axis.text.x = element_blank())
 ggsave("AllCLustFamilies.pdf")

   Clustsperday<-  Clustconversion %>% group_by(ClustID, date) %>% 
        summarise(Occurances=n())%>% ungroup %>%
        spread(., date,value=Occurances, fill=0)
    
   clustnames <- Clustsperday[,1]
   Clustsperday <- t(Clustsperday[,-1]) %>% as.data.frame
   names(Clustsperday) <- clustnames$ClustID
   
occurances <- (Clustsperday>0)*1
   #show which custers occur on which days
    orderedheat(occurances, order="none", merge=1)+
      theme(legend.position="none") + labs(title="Cluster occurance across period", x= "Day", y = "Cluster")+ 
      scale_y_continuous(breaks = 8:1, labels = levels(as.factor(Clustconversion$ClustID))) 
setwd(file.path(Figures, "Results"))
    ggsave("Clusterocurrance.pdf")

#Percentage fill of cluster if 1830 to 1900
sum(occurances[,5]|occurances[,6])/(nrow(occurances))
rm(occurances) 
```



Look at clusters across time periods using the Cluster ID's
```{r nodes in time}
setwd(SubDataSets)
NodeClust <- readRDS("NodeClust.rds")
#add in correct cluster ID's aggregate and get total number of occurances of each node per cluster
nodeclust2 <- NodeClust#testmat
rownames(nodeclust2) <-nodeclust2[,1]
nodeclust2<-nodeclust2[,-1] %>%t %>%as.data.frame %>%
 mutate(ClustID= Clustconversion$ClustID) %>% group_by(ClustID) %>% 
  summarise_each(funs(sum(., na.rm= TRUE))) %>% ungroup() 
#rownames(nodeclust2) <-nodeclust2[,1] %>%as.matrix()

#these appear to be saying that the node and cluster are independent when using the baysian method, when using the frequentist method they say that they are dependent

mi.empirical(nodeclust2[,-1])
chi2indep.empirical(nodeclust2[,-1])

#takes a long time
chisq.test(nodeclust2[,-1], simulate.p.value = TRUE) 

#This needs to be created 
mi.empirical(Clusttrans[,-1])
chi2indep.empirical(Clusttrans[,-1])

test <-chisq.test(Clusttrans[,-c(1)]) 
testratio <- test$observed/test$expected
testratio[testratio>1.5]<-1.5

chisq.test(nodeclust2[,-1]) 

orderedheat((testratio),order="none", merge=1, mid = 1, legend="Ratio")+labs(title="Ratio of observed to expected node counts", x= "Cluster", y= "Cluster")+
  scale_x_continuous(breaks = 1:8, 
                    labels = Clusttrans$ClustID)+ 
 scale_y_continuous(breaks = 8:1, 
                    labels = Clusttrans$ClustID) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
setwd(file.path(Figures, "Results"))
ggsave("ChiObsEx.pdf")

#What percentage of days are the nodes in each cluster?
#The nodes don't appear each day so need to be divided by the total number of appearences they may not the total number of days.
nodeclust3<-nodeclust2[,-1] %>%
  mutate_each(funs(./sum(.)))%>%t %>%as.data.frame


#what does a density plot of the node entropy
tempnames <-  names(nodeclust2)[-1] #transfers the node names, now tiblle prevents naming rows
NodeEntropy<-nodeclust2 %>% select(-ClustID) %>%
  summarise_each(funs(entropy)) %>% 
  gather(., key= NodeID, value= entropy)
mean(NodeEntropy$entropy)

noentropy<- nodeclust2 %>% 
  gather(., key=node, value=times, -ClustID) %>% 
  spread(., key=ClustID, value=times) %>%select(-node) %>%
  summarise_each(funs(sum)) %>% t %>% entropy()

#test whether there is a significant difference between the entropy and the marginal entropy of the clusters
t.test(x=NodeEntropy$entropy, alternative="less", mu =noentropy)

ggplot(NodeEntropy, aes(x=entropy)) +geom_density( fill="steelblue", alpha =0.7) +
  ggtitle("Density plot of Entropy") + xlab("Entropy")+geom_vline(aes(xintercept=noentropy, colour="red"))+ geom_vline(aes(xintercept=mean(NodeEntropy$entropy), colour="green"))+theme(legend.position="none") 
ggsave("stabilitydens.pdf")

 
ClusterFamilyInfo <- Clustconversion %>% 
  group_by(ClustID) %>% 
  summarise(TotNodes = sum(NodesInCluster)) %>% ungroup %>%
  mutate(PercVol = round(TotNodes/sum(TotNodes), 3), counts=n())%>%
  ungroup %>%
  mutate(rank=rank(-TotNodes))

#Heat map of whether a cluster exists on a given day
clustest <- Clustconversion%>% mutate(yesno=1) %>% 
  select(ClustID, date, yesno) %>% group_by(ClustID, date) %>%
  summarise(yesno=first(yesno)) %>% ungroup 

clustest %>% ggplot(., aes(x=date, y=ClustID))+   geom_raster() + 
    scale_fill_gradient2(low="blue",mid = "white" ,high = "red", midpoint = mid, name=legend)+theme_minimal() +

  
rm(clustest)
```





What clusters are present on each day?
```{r Cluster Plots}
Clustconversion %>% filter(ClustID!="soup", IntraDayRank<=5) %>%
  ggplot(.,aes(x=date, y= NodesInCluster, colour= as.factor(IntraDayRank))) +
  geom_line()+ggtitle("Top 5 clusters plotted by rank across all days") +
  guides(colour=guide_legend(title="Intra day rank"))+ylab("Number of nodes in Cluster")
setwd(file.path(Figures, "Method"))
ggsave("Clusterrank.pdf")

Clustsperday<- Clustconversion %>% group_by(date) %>% 
  summarise(totalclusts=n(), significantClusts=sum(NodesInCluster>50)) %>%
  ungroup

summary(Clustsperday)
sapply(Clustsperday[,-1], sd)



Clustsperday %>% gather(., Clustertype,number,-date) %>%
  ggplot(., aes(x=date, y= number, colour=Clustertype)) +geom_line()+
  ylab("Total number of Clusters")+
  ggtitle("Number of clusters a day, with total \nclusters and cluster of more than 50 nodes")
ggsave("TotalClusters.pdf")


#Plot clusters across time, with clusteres ordered by similarity
  
```





Find the counts of nodes transfering from one cluster to another across each turn by finding the the turnwise A*B matrix for the clusters in adjacent turns, add the results of that matrix multiplication to a matrix that contains all clusters

```{r Transition}
setwd(SubDataSets)
NodeClust<-readRDS("NodeClust.rds")
Clustconversion <- readRDS("Clustconversion.rds")
#names(NodeClust)[-1] <- Clustconversion$UniqueID

names(NodeClust)[-1]<- Clustconversion$UniqueID
Nodetrans <- NodeClust %>% select(-NodeID)

Nodetrans <- t(Nodetrans) %*% as.matrix(Nodetrans)

#seperate each day into clusters then superimpose them onto each other with zeros where each day isn't this means that only the interaction between consectutive days is used and nothing else.

Clusttrans <- matrix(0, ncol= max(clustdaycount), 
                   nrow= max(clustdaycount))
for(n in 1:(nrow(clustdaycount)-1)){
        #print(n)
    Clusttrans[clustdaycount[[n,4]]:clustdaycount[[n,5]],clustdaycount[[n+1,4]]:clustdaycount[[n+1,5]]] <-
      Nodetrans[clustdaycount[[n,4]]:clustdaycount[[n,5]],clustdaycount[[n+1,4]]:clustdaycount[[n+1,5]]]
    }

setwd(SubDataSets)
saveRDS(Clusttrans, "SankeyFrame.rds")
#Clusttrans <- readRDS("SankeyFrame.rds")

Clusttrans <- Clusttrans %>% as.data.frame %>% 
  mutate(ClustID = Clustconversion$ClustID) %>% group_by(ClustID) %>%summarise_each(funs(sum)) %>% ungroup %>% select(-ClustID)

Clusttrans <- Clusttrans %>% t %>% 
  as.data.frame %>% 
  mutate(ClustID = Clustconversion$ClustID) %>% 
  group_by(ClustID) %>%
  summarise_each(funs(sum)) %>% ungroup 
names(Clusttrans)[-1] <- Clusttrans$ClustID
```



Filter the node cluster data frame so that small clusters are converted into soup
```{r ConvertToSoup}

Nodenames <- NodeClust[,1]

NodeClustSoup<- NodeClust[,-1] %>%t %>% as.data.frame %>%
  mutate(ClustID=Clustconversion$ClustID) %>% 
  group_by(ClustID) %>% summarise_each(funs(sum)) %>%ungroup

names(NodeClustSoup)[-1] <- Nodenames

```

Explore the properties of the the new node cluster distribution
```{r}
#What percentage of days are the nodes in each cluster?
#The nodes don't appear each day so need to be divided by the total number of appearences they may not the total number of days.
ColumnNames<- NodeClustSoup$ClustID
NodeDistrib<-NodeClustSoup[,-1] %>%
  mutate_each(funs(./sum(.)))%>%t %>%as.data.frame
names(NodeDistrib)<-ColumnNames
rm(ColumnNames)
setwd(SubDataSets)
saveRDS(NodeDistrib, "NodeDistrib.rds")

tempnames <-  names(NodeClustSoup)[-1]
#what does a density plot of the node stability look like?
Nodestab <-NodeClustSoup[,-1] %>%
  mutate_each(funs((./sum(.))^2))%>%t %>%as.data.frame %>%
  mutate(stability = rowSums(.)) %>%
  select(stability)%>%mutate(NodeID = tempnames) 

ggplot(Nodestab, aes(x=stability)) +geom_density()
```

create sankey plot, alluvial diagram
```{r Sankey}

#these plots cannot be programmatically saved, they have to be done manually using the export button. they cannot be exported very well in AWS either.
setwd(SubDataSets)
SankeyFrame<-readRDS("SankeyFrame.rds") %>% as.data.frame
Clustconversion <- readRDS("Clustconversion.rds")


setwd(file.path(Figures, "Results"))
SankeyData(95:101,SankeyFrame, Clustconversion, SOUP = TRUE)
SankeyData(95:101,SankeyFrame, Clustconversion, SOUP = FALSE)
SankeyData(102:108,SankeyFrame, Clustconversion, SOUP = FALSE)


test <-SankeyData(95:101,SankeyFrame, Clustconversion, simple = F)
test2<- test$SankeyNodes
Sankey2<- test$Sankey2

 sankeyNetwork(Links=test$Sankey2,
                     Nodes=test$SankeyNodes,
                     NodeID = "UniqueID",
                     Source="source",
                     Target= "target",
                     Value = "Nodes",
                     NodeGroup = "ClustID",
                     fontSize = 0,
                     colourScale = JS(test$SankeyColours),
                     iterations = 0)

```

Make a graph of the transition matrix.
```{r Clustertransgraph}
clustrans2 <- Clusttrans[,-1]/rowSums(Clusttrans[,-1]) 

clustrans3 <- Clusttrans
clustrans3[,-1] <- clustrans3[,-1]/rowSums(clustrans3[,-1]) 

setwd(TexTables)
#To go into the document smoothly it needs to be a tabular.
xtable(clustrans3, caption="The cluster transition probabilities",
       label = "tab:clustrans") %>% 
  print(., type="latex", file="clustrans.tex")
rm(clustrans3)

#is the probability of transition random?
t.test(clustrans2^2 %>% rowSums, mu=0.1 )
#apparently not

#Clusttrans[-1,-1]
Clustertransgraph = graph.adjacency(clustrans2, 
                          mode = "directed",
                          weighted = TRUE, diag = TRUE)

ecount(Clustertransgraph)
Clustertransgraph<- set.vertex.attribute(Clustertransgraph, 
                                         "size",
                     index = V(Clustertransgraph),
                     value=
                       rowSums(Clusttrans)*50/max(rowSums(Clusttrans)))


get.vertex.attribute(Clustertransgraph,"size")
setwd(SubDataSets)
write.graph(Clustertransgraph, 
            "Clustertransgraph.graphml", 
            format = "graphml")

#Clustertransgraph<- read.graph("Clustertransgraph.graphml",format = "graphml")


clus_heat <- bigheat(t(clustrans2)[,10:1], merge=1, mid=0.1, legend="Probability") 

clus_heat +labs(title = "Transition Matrix", 
      x = "Cluster ID", 
      y = "Cluster ID") +
 scale_x_continuous(breaks = 1:10, 
                    labels = Clusttrans$ClustID)+ 
 scale_y_continuous(breaks = 10:1, 
                    labels = Clusttrans$ClustID) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
setwd(file.path(Figures, "Results"))
ggsave("Clusttransgraph.png")
```



#Comparing load profiles across communities
```{r dayloadprofile}
setwd(SubDataSets)
Clustconversion<- readRDS("Clustconversion.rds")
dayinit <-"2011-08-09"
daydata <- CreateLoadProfile2(dayinit)
#make new CLuster Profile function that loads the node data then assigns node cluster relationship from the cluster

setwd(file.path(Figures,"Results"))
ggplot(daydata, aes(x=time, y=sum, colour= ClustID)) + geom_line() +   ggtitle("Sum Cluster Profiles")
ggsave("singledaypatternSum.pdf")

ggplot(daydata, aes(x=time, y=mean, colour= ClustID)) + geom_line() + 
  ggtitle("Mean Cluster Profiles")
ggsave("singledaypatternMean.pdf")

ggplot(daydata, aes(x=time, y=median, colour= ClustID)) + geom_line() + 
  ggtitle("Median Cluster Profiles")
ggsave("singledaypatternMedian.pdf")

ggplot(daydata, aes(x=time, y=sd, colour= ClustID)) + geom_line() + 
  ggtitle("Standard deviation of Cluster Profiles")
ggsave("singledaypatternVar.pdf")

totalnodes <-daydata %>% group_by(ClustID) %>% summarise(count = first(n))
setwd(TexTables)
#To go into the document smoothly it needs to be a tabular.
xtable(totalnodes, caption=c("The total number of nodes in in each cluster for 09-08-2011", "Nodes per cluster"), 
       label= "table:NodesPerCluster") %>% 
  print(., type="latex", file="totalnodes.tex")

#there are two different ways of ID'ing clusters. using nodes similarity or using cluster behavioural pattern.
setwd(daytimeseries)


test <- CreateLoadProfile3(dayinit)
setwd(file.path(Figures,"Results"))
test %>% 
  filter(ClustID=="1600"|ClustID=="1830"|ClustID=="2100") %>%
     ggplot(., aes(x=time,y=kwh,group=NodeID, colour=(ClustID))) +
     geom_line(alpha=0.1)+
     facet_grid(ClustID~.)+
     ggtitle("Load Profiles of all Nodes within a Cluster") +
     scale_color_manual(values= ggplotColours(10)[c(4,7,8)], guide=FALSE)
setwd(file.path(Figures,"Results"))
ggsave("NodeProfilesInCluster.pdf")


test %>% filter(ClustID!='soup')%>%
  ggplot(., aes(x=time,y=kwh,group=NodeID, colour=ClustID)) +
     geom_line(alpha=0.1) + ggtitle("All nodes Coloured by Cluster")
setwd(file.path(Figures,"Appendix"))
ggsave("AllNodesClustercolour.pdf")


```

Is there a pttern of people moving through clusters together?
as the clusters that are formed are essentially a temproal shift + the soup  I can use corellation where the cluster ID's are converted to numbers?
```{r similar nodes}
setwd(SubDataSets)
NodeClust <-readRDS("NodeClust.rds")
Clustconversion <- readRDS("Clustconversion.rds")

#This is defintatly not working properly
test <-mclapply(2:ncol(NodeClust), function(n){
      test <- NodeClust[,c(1,n)] %>% 
        gather(., key=UniqueID, value=yesno, -NodeID)%>% 
        filter(yesno==1) %>% select(-yesno)
      return(test)
      }, 
      mc.cores = detectCores()) %>% 
  bind_rows()


test2 <- Clustconversion %>%select(ClustID, UniqueID, date) %>%left_join(test,., by="UniqueID") %>% select(-UniqueID) %>%
mutate(ClustID=as.factor(ClustID) %>% as.integer) 
  

test3<-  mclapply(unique(test2$NodeID), function(n){
 test3 <- test2 %>% filter(NodeID==n)%>%  
spread(key= date, value=ClustID)
}) %>% bind_rows()



NodeCor <- select(test3, -NodeID) %>% t %>% cor

names(NodeCor) <- test3$NodeID

x <- orderedheat(NodeCor, order = "both", simMat = TRUE)

cutoff <- data.frame(cutoff = seq(0.1,1,0.05), edges = NA,nodes=NA)
distmat <- abs(NodeCor)

#what cutoff for the graph of node similarity
    test <- lapply(cutoff[,1] ,function(n) {
      logicmat<-distmat>n  
      data.frame(cutoff = n, 
                 edges = sum(logicmat, na.rm = TRUE),
                 isolated =sum(rowSums(logicmat,na.rm=TRUE)>0))
    }
  ) %>% bind_rows()
    
    
NodeSimGraph <- createcleangraph2(NodeCor, NodeCor>0.3)
NodeSimGraph <-  delete.vertices(NodeSimGraph ,
                                 which(degree(NodeSimGraph)<1))
NodeSimGraph <- detectcomms(NodeSimGraph)   

setwd(SubDataSets)
write.graph(NodeSimGraph, 
            file="NodeSimGraph.graphml",
            format ="graphml" )



#using the clusters there is an extremly weak relationship with cluster movement,


cleandata <- readRDS("cleandatafilled.rds")

Nodecor2 <- bigcor(cleandata[,-1], par= TRUE) %>%as.ffdf %>%
                       as.data.frame
Nodecor2 <- Nodecor2%>%as.ffdf %>%
                       as.data.frame
x <- orderedheat(NodeCor2, order = "both", simMat = TRUE, mid = 0)

cutoff <- data.frame(cutoff = seq(0.1,1,0.05), edges = NA,nodes=NA)
distmat <- abs(NodeCor2)

#what cutoff for the graph of node similarity
    test <- lapply(cutoff[,1] ,function(n) {
      logicmat<-distmat>n  
      data.frame(cutoff = n, 
                 edges = sum(logicmat, na.rm = TRUE),
                 isolated =sum(rowSums(logicmat,na.rm=TRUE)>0))
    }
  ) %>% bind_rows()

NodeSimGraph2 <- createcleangraph2(NodeCor2, NodeCor2>0.3)
NodeSimGraph2 <-  delete.vertices(NodeSimGraph2 ,
                                 which(degree(NodeSimGraph2)<1))
NodeSimGraph2 <- detectcomms(NodeSimGraph2)   

setwd(SubDataSets)
write.graph(NodeSimGraph2, 
            file="NodeSimGraph2.graphml",
            format ="graphml" )

```


Make a predicitive model using a transition matrix and one step ahead prediction
```{r Bayes Pred}
setwd(SubDataSets)
PredTrans <- readRDS("SankeyFrame.rds")
Clustconversion <- readRDS("Clustconversion.rds")
clusterprofile <- readRDS("clusterprofile.rds")


#start day
daynumber <- 200
date <- (Clustconversion %>% filter(day==daynumber))$date %>% unique

#Create a transition matrix for the first 200 days and use this matrix to make predictions on the remaining 43 days 

PredTrans <- PredTrans %>% as.data.frame %>% bind_cols(select(Clustconversion, ClustID, day),.)%>% filter(day<=daynumber)%>% select(-day) %>% group_by(ClustID) %>%summarise_each(funs(sum)) %>% ungroup %>% select(-ClustID)


PredTrans <- PredTrans %>% t %>% 
  as.data.frame %>% 
 bind_cols(select(Clustconversion, ClustID, day),.)%>% filter(day<=daynumber)%>% select(-day)%>% 
  group_by(ClustID) %>%
  summarise_each(funs(sum)) %>% ungroup 

names(PredTrans)[-1] <- PredTrans$ClustID

PredTrans <- PredTrans %>% select(-ClustID)

#Final step of creating the transition matrix
PredTrans <- (PredTrans/rowSums(PredTrans)) %>% as.matrix
saveRDS(PredTrans, "PredTrans.rds")

#ensure that all clusters are present in the data for each day
AllClusts <- unique(Clustconversion$ClustID)%>%
  data.frame(ClustID=.) %>% arrange(ClustID)


#Matrix predicting the number of nodes in each cluster
#Uses days uses days X to last day minus 1 to predict x+1 to last day
predictmat <- lapply(daynumber:(max(Clustconversion$day)-1), function(n){

    source <-Clustconversion %>% 
      filter(day ==n) %>% group_by(ClustID)%>%
      summarise(NodesInCluster=sum(NodesInCluster)) %>% ungroup %>%
      left_join(AllClusts,., by="ClustID")
    source[is.na(source)] <-0
    
    predicted <- (t(source$NodesInCluster) %*% PredTrans) %>%as.data.frame
}) %>% bind_rows()



#Create a matrix of the target/actual number of Nodes in each cluster per day
targetmat <- lapply((daynumber+1):max(Clustconversion$day), function(n){
target <-Clustconversion %>% filter(day ==n) %>% group_by(ClustID)%>%
  summarise(NodesInCluster=sum(NodesInCluster)) %>% ungroup %>%
  left_join(AllClusts,., by="ClustID")
target[is.na(target)] <-0

targ2 <- t(target$NodesInCluster) %>% as.data.frame
names(targ2) <- target$ClustID
return(targ2)

}
) %>% bind_rows()


#vector of days 201 to 243
datevect <- (Clustconversion %>% filter(day>=daynumber))$date %>% unique

#The actual mean profile for the the nodes in the test set
#takes a while
DayProfile <- DayProfilemat(unique(Clustconversion$date), AllClusts)

#Filter so only the correct dates are still there
#This hack forword fills the missing clusters with data from the previously occuring cluster.
DayProfile<-DayProfile%>% group_by(ClustID) %>%
  mutate_at(vars(matches("0")), funs(na.locf)) %>% 
  filter(date>=min(datevect)) %>% ungroup



#Join the predicted node amounts to the date the prediction is for and gather the resulting data frame
predictmat2 <- bind_cols(data.frame(date=datevect[-1]), predictmat) %>%   gather(., key = ClustID, value = Nodes,-date)%>% arrange(date)

targetmat2 <- bind_cols(data.frame(date=datevect[-1]), targetmat) %>% gather(., key=ClustID, value= Nodes, -date) %>% arrange(date)

# get the Load profile of each cluster
#this should be the actual load profile
test2 <-DayProfile %>% filter(date != min(datevect)) %>%
  mutate_at(.,vars(matches("0")),funs(.*targetmat2$Nodes))
  
subnumber <- nrow(AllClusts) %>% as.numeric
#Find the predicted load by multiplying the predicted cluster volumes by the mean load profile for the previous day.
PredLoad<-DayProfile %>% filter(date != min(datevect)) %>%
  mutate_at(.,vars(matches("0")),
            funs(.*lag(predictmat2$Nodes,
                       subnumber)))%>% 
  select(-ClustID)%>%group_by(date) %>% 
  summarise_all(funs(sum))
rm(subnumber)

# Get the predicted profile of each cluster
DayProfile2 <- test2 %>% select(-ClustID)%>%group_by(date) %>% 
  summarise_all(funs(sum))


#dates have to be shifted 1 day as the prediction is one day ahead
colourframe <- data.frame(date=datevect) %>% mutate(colour = ifelse(date=="2011-12-25","#FF0000","#000000"),colour = ifelse(date=="2011-12-24","#33ff00", colour))


results <-left_join(
  gather(DayProfile2, key= time, value=Actual, -date),
              gather(PredLoad, key= time, value=Predicted, -date),
          by=c("date", "time")) %>%
  mutate(residuals= Actual-Predicted) %>%
  filter(!is.na(Predicted)) %>% 
  mutate(PercErr = abs(residuals/Actual))

mape <- function(yhat, y){
mean(abs((y - yhat)/y))}

#The results
postResample(results$Predicted, results$Actual)
mape(results$Predicted,results$Actual)

#date of maximum error is the date that was artificially filled in.
results$date[which.max(results$PercErr)]

ResNox <- results %>% filter(date != results$date[which.max(results$PercErr)])

postResample(ResNox$Predicted, ResNox$Actual)
mape(ResNox$Predicted,ResNox$Actual)


```


```{r Plot Model Res}
ggplot(results, aes(x=time,y= residuals, group = date, colour= as.factor(date))) +
  geom_line() +  
  scale_colour_manual(values=colourframe$colour)+
  theme(legend.position="none") +
  labs(title= "Residuals across prediction period",
       x= "Time", 
       y="Residuals (kwh)")
setwd(file.path(Figures, "Results"))
ggsave("ResidsWXmas.pdf")


 results%>% ggplot(., aes(x=time,y= PercErr, 
       group = date, colour= as.factor(date))) +
  geom_line() +   
  scale_colour_manual(values=colourframe$colour)+
  theme(legend.position="none") +
  labs(title= "Percentage error ",
       x= "Time", 
       y="Percentage Error")
setwd(file.path(Figures, "Results"))
ggsave("AbsPercErrLine.pdf")
 
 
results %>%select(-time) %>% #group_by(date) %>%
  #summarise_each(funs(sum)) %>% mutate(PercErr= residuals/Actual) %>%
  ggplot(., aes(x= abs(PercErr))) + 
  geom_density(fill="steelblue", alpha =0.7)+
    labs(title= "Absolute percentage error density",
       x="Percentage Error")
setwd(file.path(Figures, "Results"))
ggsave("AbsPercErrDistrib.pdf")

results %>%  ggplot(., aes(x= Actual, y= Predicted, colour= PercErr)) +
  geom_point() +xlim(1200,+2500)+ylim(800,2500)+
  geom_abline(intercept = 0, slope = 1) +
  ggtitle("Actual vs predicted results coloured\n in terms of percentage error") +
    scale_colour_gradient2(low="blue",mid = "white" ,high = "red", midpoint = mean(results$PercErr))
setwd(file.path(Figures, "Results"))
ggsave("ActualVsPredErr.pdf")


               
results %>%  ggplot(., aes(x= time, y= PercErr, fill= time)) +
  geom_boxplot()+ theme(legend.position="none") +
  ggtitle("Error distribution across time periods")
setwd(file.path(Figures, "Results"))
ggsave("BoxTimeErr.pdf")

results %>% group_by(date) %>% 
  summarise(residuals = sum(residuals))%>% 
  ggplot(., aes(x=date, y=residuals, group=1))+geom_line()+
  ggtitle("Total residuals per day")
setwd(file.path(Figures, "Results"))
ggsave("ResidualsTime.pdf")

```


```{r Lin Pred}
setwd(SubDataSets)
cleandata<-readRDS("cleandatafilled.rds")

Portfolio <- cleandata %>% select(-Date.Time) %>% rowSums %>% 
  data.frame(Date.Time= cleandata[,1], kwh = .) %>% 
  mutate(date= as.Date(Date.Time), 
         time = format(Date.Time, format="%H:%M"),
         weekday = weekdays(Date.Time),
         month = months(Date.Time)) %>%
  mutate(daylag= lag(kwh,1),
         weeklag = lag(kwh,7) )

moddat <- Portfolio %>% select(kwh,daylag, weeklag, date, time) %>% filter(!is.na(weeklag))

datex <-date

train <-moddat %>% filter(date <=datex)

#lags by
testmod <-moddat %>% filter(date >datex)

mod1 <- train(x= train[,2:3], y= train[,1], method="lm")

mod2 <- xgboost(data= as.matrix(train[,2:3]), label= train[,1],
                nround = 200,
                objective= "reg:linear")


linpreds <- predict(mod1, testmod[,2:3])
linpreds2 <- predict(mod2, as.matrix(testmod[,2:3]))


LinearMod <- c(postResample(linpreds, testmod[,1]),
               mape(linpreds, testmod[,1] ))%>% as.data.frame
LinearMod 
mape(linpreds, testmod[,1] )

postResample(linpreds2, testmod[,1])
mape(linpreds2, testmod[,1] )


#what is the difference between the dayprofile output and the actual observed load

moderr <- testmod %>% select(kwh, date, time) %>% 
            left_join(results, ., by =c("date", "time"))


moderr %>% select(-residuals, -PercErr, -kwh) %>% 
  gather(., key = type, value =kwh, -date, -time) %>%
  ggplot(., aes(x=time, y=kwh, group=date)) +facet_grid(.~type) +
  geom_line() +ggtitle("Difference between actual and predicted results") +theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position="none")
setwd(file.path(Figures, "Results"))
ggsave("ActualVsPredLine.pdf")

moderr %>% ggplot(., aes(y=kwh,x= time, group= date))+
  geom_line()+ggtitle("Actual results")

moderr %>% ggplot(., aes(y=Predicted,x= time, group= date))+
  geom_line()+("Predicted Results")


#check bayesian model
test2 <- left_join(results, testmod, by=c("date", "time"))
TransMod <-c(postResample(results$Predicted,
                          test2$kwh),
             mape(results$Predicted,test2$kwh) ) %>% as.data.frame 

modcom <-  bind_cols(TransMod, LinearMod) 
names(modcom) <- c("TransMod", "LinearMod")

setwd(TexTables)
modcom %>% mutate(type= c("RMSE", "R^2", "MAPE")) %>% 
  gather(., key="model", value= "score", -type ) %>%
  spread(., key=model, value=score) %>%
  xtable(., caption="Performance metrics for the two model types",
         label="tab:modperf",digits=c(3,3,2,2))%>% 
  print(., type="latex", file="modperf.tex")


```


```{r Node transfer}
setwd(SubDataSets)

#The distribution of nodes
NodeDistrib<- readRDS("NodeDistrib.rds")

#Finds what node is in which cluster each day
NodeClust2 <-readRDS("NodeClust2.rds")


#find out which days have the same clusters
sameclusts <- sapply(2:(ncol(NodeClust2)-1), function(n){
    day1 <-NodeClust2[,n-1] %>% unique 
    day2 <-NodeClust2[,n] %>% unique
    
    all.equal(day1[order(day1)],day2[order(day2)])[1]==TRUE
    }
)

which(c(FALSE, sameclusts)) %>% length

names(NodeClust2[,which(c(FALSE, sameclusts))])

setwd(TexTables)
data.frame(`First Pair` =names(NodeClust2[,c(sameclusts, FALSE)]),
           `Matching Pair` =names(NodeClust2[,c(FALSE, sameclusts)])) %>%
  xtable(., caption="Day pairs with matching clusters",
       label = "tab:DayPairs") %>% 
  print(., type="latex",tabular.environment="longtable", file="DayPairs.tex")


smarttable <-names(nodeclust2)[-1] %>% sub("X", "", .)%>% as.integer %>% .[order(.)]%>% matrix(., ncol=10, byrow=TRUE) %>% as.data.frame %>%
  xtable(., caption="Smart meter ID's used in the project",
       label = "tab:SMARTID") 
align(smarttable) <- "|r|rrrrrrrrrr|"

  print(smarttable, type="latex",tabular.environment="longtable", file="SMARTID.tex", floating =FALSE)


#days 18, 19, 20  has all 8 clusters
           NodeClust2[,23] %>% unique 
    day1 <-NodeClust2[,19] %>% unique 
    day2 <-NodeClust2[,20] %>% unique

CorrectClust2 <- mclapply(1:length(which(c(FALSE, sameclusts))), function(m){
p <-which(c(FALSE, sameclusts))[m]

CorrectClust <- NodeClust2[,c(1,p-1,p)]

clustnumres <- sapply(1:nrow(CorrectClust) , function(n){
  #the colnames are chosed as PredTrans doesn't have row names but they would be the same
clustnumres <-matrix(1, nrow=1, ncol=ncol(PredTrans)) %*% ((colnames(PredTrans) == CorrectClust[n,2])  *PredTrans) * NodeDistrib[n,]
names(clustnumres)[which.max(clustnumres)]

  } )
CorrectClust <- CorrectClust %>% mutate(Predicted = clustnumres)

}, mc.cores = detectCores() )

CorrectClust2 <- lapply(CorrectClust2, function(n){
  df <- n
  names(df)[2:3]<- c("source", "target")
df
  }) %>% bind_rows()

confmat <- confusionMatrix(CorrectClust2[,3], CorrectClust2[,2])

#Does converting it to a regression work better?... No
postResample(as.numeric(as.factor(CorrectClust2[,2])),
as.numeric(as.factor(CorrectClust2[,3])))

z <-as.matrix(confmat$table/rowSums(confmat$table))%>% 
  as.data.frame %>% 
  spread(., key= Reference, value=Freq) %>% select(-Prediction)
  
orderedheat(t(z), order="none", merge=1, mid=0.125) +
  theme(legend.position="none" ) +
  labs(title= "Transition matrix results", x= "Clusters", y="Clusters")+
  scale_y_continuous(breaks = 8:1, labels = levels(as.factor(Clustconversion$ClustID))) +
  scale_x_continuous(breaks = 1:8, labels = levels(as.factor(Clustconversion$ClustID))) 
setwd(file.path(Figures,"Results"))
ggsave("Transitionresults.pdf")


rm(z)

```



#Do the simple and explanded XGboost model

Create Data frame

```{r XGframe}

test  <-mclapply(9:247, function(n) {
    e<-n
    f<-e-7
    test <- NodeClust2[,f:e]
    names(test)<- c(paste("day",1:7, sep="" ),"target")
    test %<>% mutate(NodeID = NodeClust2$NodeID)
    return(test)
  }) %>% bind_rows


Nodeclustframe <- NodeDistrib  %>%
  mutate_all(funs(cut.default), 
                            breaks= 20, 
                            labels= seq(0.05,1,0.05) ) %>% 
  mutate(NodeID = rownames(NodeDistrib)) %>% left_join(test, ., by="NodeID", .) %>% select(-NodeID)

names(Nodeclustframe) <- make.names(names(Nodeclustframe))

targvect <- Nodeclustframe %>% select(target)%>% 
  mutate(target = as.factor(target), targetnum = as.numeric(target))

set.seed(1928)
Trainvect <-sample(1:nrow(test),nrow(test)*0.8)

```

Do simple XGboost model
```{r XGsimple}

hashedframe <-Nodeclustframe%>% select(-target)%>% 
  select(day7:soup) %>% 
  slice(Trainvect) %>%
  hashed.model.matrix(~., .)

hashedframeTest <-Nodeclustframe%>% select(-target)%>% 
  select(day7:soup) %>% 
  slice(-Trainvect) %>%
  hashed.model.matrix(~., .)

NodeXGClustSimple <- xgboost(hashedframe,
                    targvect[Trainvect,2], 
                    max.depth=7, 
                    eta=0.1,
                    nround = 100, 
                    objective = "multi:softmax",
                    num_class = 9,
                    verbose = ifelse(interactive(), 1, 0),
                    eval_metric ="merror")
setwd(SubDataSets)
saveRDS(NodeXGClust, "NodeXGClustSimple.rds")

NodeClustPredsSimple <- predict(NodeXGClustSimple, hashedframeTest)

confXGSimple <- confusionMatrix(NodeClustPredsSimple,targvect$targetnum[-Trainvect] )

confXGSimple
z2 <-as.matrix(confXGSimple$table/rowSums(confXGSimple$table))%>% 
  as.data.frame %>% 
  spread(., key= Reference, value=Freq) %>% select(-Prediction)

  orderedheat(t(z2), order="none", merge=1, mid=0.125)+
  theme(legend.position = "none") +
  labs(title = "Simple XGboost results", x= "Clusters", y= "Clusters")+
  scale_y_continuous(breaks = 8:1, labels = levels(as.factor(Clustconversion$ClustID))) +
  scale_x_continuous(breaks = 1:8, labels = levels(as.factor(Clustconversion$ClustID))) 

setwd(file.path(Figures,"Results"))
ggsave("XGboostresultsSimple.pdf")

```


```{r XGclusterExp}

hashedframe <-Nodeclustframe%>% select(-target)%>%
                                    slice(Trainvect) %>%
  hashed.model.matrix(~., .)

hashedframeTest <-Nodeclustframe%>% select(-target)%>%
                                    slice(-Trainvect) %>%
  hashed.model.matrix(~., .)

NodeXGClust <- xgboost(hashedframe,
                    targvect[Trainvect,2], 
                    max.depth=7, 
                    eta=0.1,
                    nround = 100, 
                    objective = "multi:softmax",
                    num_class = 9,
                    verbose = ifelse(interactive(), 1, 0),
                    eval_metric ="merror")
setwd(SubDataSets)
saveRDS(NodeXGClust, "NodeXGClust.rds")

NodeXGClust <- readRDS("NodeXGClust.rds")

NodeClustPreds <- predict(NodeXGClust, hashedframeTest)

confXG <- confusionMatrix(NodeClustPreds,targvect$targetnum[-Trainvect] )

confXG
z2 <-as.matrix(confXG$table/rowSums(confXG$table))%>% 
  as.data.frame %>% 
  spread(., key= Reference, value=Freq) %>% select(-Prediction)

  orderedheat(t(z2), order="none", merge=1, mid=0.125)+
  theme(legend.position = "none") +
  labs(title = "XGboost results", x= "Clusters", y= "Clusters")+
  scale_y_continuous(breaks = 8:1, labels = levels(as.factor(Clustconversion$ClustID))) +
  scale_x_continuous(breaks = 1:8, labels = levels(as.factor(Clustconversion$ClustID))) 

setwd(file.path(Figures,"Results"))
ggsave("XGboostresults.pdf")

```


Clean data daily profiles

Portfolio load and distributions
```{r portfolio Load}


Portfolio %>% group_by(time, weekday) %>% summarise(kwh= mean(kwh), sd = sd(kwh)) %>% mutate(group=1, lsd= kwh-sd(kwh), usd=kwh+sd(kwh))%>% 
  ggplot(., aes(x=time, y= kwh, group= weekd
                ay)) +
#  geom_ribbon(aes(ymin=lsd, ymax=usd), fill= "grey70")+
  geom_line(aes(colour= weekday)) +ggtitle("Mean evening profile for each weekday")
setwd(file.path(Figures, "Results"))
ggsave("PortfolioLoad.pdf")

Portfolio %>% group_by(date, month) %>% summarise(kwh= sum(kwh)) %>% 
  ungroup%>%
  mutate(group=1, 
         month =factor(month,levels=unique(p2$month) ))%>% 
  ggplot(., aes(x=month, y= kwh, group= month)) +
  geom_boxplot(aes(fill= month)) +ggtitle("Daily consumption across months")  + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position="none")
ggsave("DailyConsumptionMonths.pdf")

```


Add in MOSAIC class and create some plots and a really terrible predictive model
```{r Mosaic}
setwd(datafile)
Customerdata<-read.csv("CustomerTestCellDefinition.csv") %>% 
  mutate(NodeID =make.names(Location.ID), MosaicNumber = as.factor(Mosaic.Class) %>% as.numeric) %>% 
  select(-Location.ID, -Customer.Type)

table(Customerdata$Tariff.Type)
table(Customerdata$Mosaic.Class)
table(Customerdata$In.Out.Region)

x <-NodeDistrib %>%mutate_all(funs(.^2)) %>%
  mutate(NodeID =rownames(.)) %>% left_join(., select(Customerdata, Mosaic.Class, NodeID), by="NodeID")

#any mosaic pattern?
ggplot(x, aes(colour= as.factor(Mosaic.Class), x= stability)) +geom_density()
ggplot(x, aes(x= as.factor(Mosaic.Class), y= stability)) +geom_boxplot()


NDM <-NodeDistrib %>% mutate(NodeID = NodeClust$NodeID) %>% 
  left_join(., select(Customerdata, Mosaic.Class,MosaicNumber, NodeID), by="NodeID")%>%
  select(-NodeID) 


NDM %>% group_by(Mosaic.Class, MosaicNumber) %>% summarise(coutns= n())
```




```{r Node day Cluster}
setwd(SubDataSets)
NodeDistrib <- readRDS( "NodeDistrib.rds")

NodeClust2 <- NodeClust
names(NodeClust2)[-1]<- Clustconversion$UniqueID
NodeClust2 %<>% gather(., key=UniqueID, value=present, -NodeID) %>%filter(present==1) %>% select(-present) %>% left_join(., Clustconversion, by="UniqueID") %>%select(NodeID, date, ClustID)


#duplicates have to be removed. why are they there anyway? this needs to be investigated.
df_dups <- NodeClust2[c("NodeID", "date")]

#after changes it looks like the duplication issue has been resolved
NodeClust2<-NodeClust2%>% filter(!duplicated(df_dups))%>% spread(.,key=date, value=ClustID)
setwd(SubDataSets)
saveRDS(NodeClust2, "NodeClust2.rds")

test <- NodeClust2 %>% 
  left_join(., select(Customerdata, NodeID, MosaicNumber), by="NodeID") %>%
  select(-NodeID)
Mosaictrain<-test$MosaicNumber[idx]
Mosaictest<-test$MosaicNumber[-idx]

names(test)<-make.names(names(test))
trainx <- hashed.model.matrix(~.,data=select(test[idx,], -MosaicNumber))
testx <- hashed.model.matrix(~.,data=select(test[-idx,], -MosaicNumber))

XGdayClass <- xgboost(trainx, Mosaictrain, 
                    max.depth=5, 
                    eta=0.10,
                    nround = 100, 
                    objective = "multi:softmax",
                    num_class = 17,
                    verbose = ifelse(interactive(), 1, 0),
                    eval_metric ="merror")

XGpreds <- predict(XGdayClass, testx) 

XGconf <- confusionMatrix(XGpreds, Mosaictest)
XGconf
#best performers 5 7 10 13



setwd(datafile)
datasource <- read.csv("HalfHourlyDataSource.csv")

unique(NodeClust2$NodeID)%>% length

test<- Clustconversion %>%group_by(ClustID) %>%summarise(NodesInCluster=mean(NodesInCluster), occurances=n(), date= first(date)) %>%filter(occurances==1)
```



#rubbish Xgboost model detecting class.
```{r XGBoostmodel}

NDM <- NodeDistrib  %>%
  mutate_all(funs(cut.default), 
                            breaks= 20, 
                            labels= seq(0.05,1,0.05) ) %>% 
  mutate(NodeID = rownames(NodeDistrib)) %>% 
  left_join(NodeClust2, ., by="NodeID", .) %>% 
  left_join(., select(Customerdata, Mosaic.Class,MosaicNumber, NodeID), by="NodeID") %>% mutate(MosaicNumber= as.factor(MosaicNumber)) %>%
  select(-NodeID)  

names(NDM) <- make.names(names(NDM))


set.seed(1928)
#Trainvect <-sample(1:nrow(NDM),nrow(NDM)*0.8)
Trainvect <- createDataPartition(make.names(NDM$MosaicNumber), p=0.7, list=FALSE)

hashedframe <-NDM%>% select(-Mosaic.Class, -MosaicNumber)%>%
                                    slice(Trainvect) %>%
  hashed.model.matrix(~., .)

hashedframeTest <-NDM%>% select(-Mosaic.Class, -MosaicNumber)%>%
                                    slice(-Trainvect) %>%
  hashed.model.matrix(~., .)


NorXGtre <- xgboost(hashedframe,
                    NDM$MosaicNumber[Trainvect], 
                    max.depth=7, 
                    eta=0.1,
                    nround = 100, 
                    objective = "multi:softmax",
                    num_class = 17,
                    verbose = ifelse(interactive(), 1, 0),
                    eval_metric ="merror")


XGpreds <- predict(NorXGtre, hashedframeTest) 
levels(XGpreds) <- levels(NDM$MosaicNumber)
XGconf <- confusionMatrix(XGpreds, NDM$MosaicNumber[-Trainvect])
XGconf


test <- XGconf$table %>% data.frame() %>% spread(., key=Reference, value=Freq)

Customerdata %>% group_by(MosaicNumber) %>%summarise(Mosaic.Class=first(Mosaic.Class))

#is there anything useful here?....No
orderedheat(test[,-1]/rowSums(test[,-1]), merge=1)
```




This code chunk makes a mask that can be used as an example in the method section.
```{r Example mask}


blocks <- 6
IDvect <- Clustconversion$day[Clustconversion$day <=blocks]

test <-matrix(0, length(IDvect), length(IDvect))


for( i in 2:blocks){
  test[IDvect==i-1,IDvect==i]<-1
  
}

maskex <- orderedheat(t(test), order = "none", merge = 1)
maskex+
  ggtitle("Example masking matrix") + 
  theme(legend.position="none", 
        axis.text.x  = element_blank(), 
        axis.text.y = element_blank())+   
  xlab("Target Clusters") +
  ylab("Source Clusters")
setwd(file.path(Figures, "Method"))
ggsave("ClusterMask.pdf")

```

Energy breakdown
```{r}
setwd(file.path(Figures, "Appendix"))
data.frame(fuel = c("Gas", "Solid", "Oil", "Electricity"),
           Consumption = c(34217, 2720, 1051, 10036)) %>%
  mutate(Consumption = Consumption/sum(Consumption)) %>%
  ggplot(., aes(x=fuel, y= Consumption)) +
  geom_bar(stat = "identity") + 
  ggtitle("Percentage domestic electricity \n consumption broken out by fuel type (kToE)") 
ggsave("DomElConsumFuel.pdf")

data.frame(fuel = c("SpaceHeating", "Water", "Cooking", "LightingAppliances"),
           Consumption = c(29061, 11329,1317,6317)) %>%
  mutate(Consumption = Consumption/sum(Consumption)) %>%
  ggplot(., aes(x=fuel, y= Consumption)) +
  geom_bar(stat = "identity") + 
  ggtitle("Energy consumption by end of use (kToE)") 
ggsave("DomConsumType.pdf")

data.frame(fuel = c("SpaceHeating", "Water", "Cooking", "LightingAppliances"),
           Consumption = c(1565, 1556,597,6317)) %>%
  mutate(Consumption = Consumption/sum(Consumption)) %>%
  ggplot(., aes(x=fuel, y= Consumption)) +
  geom_bar(stat = "identity") + 
  ggtitle("Electricity consumption by end of use (kToE)") 
ggsave("DomElConsumType.pdf")

```

Create bibtex object to cite packages
```{r}



```

